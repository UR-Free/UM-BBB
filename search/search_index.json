{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to UM-BBB UM-BBB\u662f\u672c\u4eba\u5728Uni-Mol\u57fa\u7840\u4e0a\u7528\u4e8e\u9879\u76ee\u7ec3\u4e60\u7684\u5b9e\u8df5\u4ea7\u7269\uff0c\u5173\u4e8eUni-Mol\u7684\u8be6\u7ec6\u4fe1\u606f\u8bf7\u8bbf\u95ee uni-mol\u5b98\u65b9\u7f51\u5740 . \u4f5c\u8005\u4fe1\u606f \u59d3\u540d \u2003\u6768\u5408 \u6559\u80b2\u80cc\u666f \u2003\u672c\u79d1 \u4e2d\u5c71\u5927\u5b66 \u836f\u5b66\u9662 \u836f\u5b66\u4e13\u4e1a \u2003\u7855\u58eb \u56fd\u79d1\u5927 \u6df1\u5733\u5148\u8fdb\u6280\u672f\u7814\u7a76\u9662 \u8ba1\u7b97\u751f\u7269\u5b66\u65b9\u5411 \u9879\u76ee\u80cc\u666f\u4e0e\u76ee\u6807 Uni-Mol\u7b80\u4ecb \u3000\u3000\u4ece\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u6570\u636e\u4e2d\u63d0\u53d6\u8868\u5f81\u4fe1\u606f\uff0c\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6b63\u5728\u5e2d\u5377AI\u9886\u57df\uff0c\u5e76\u6210\u4e3a\u5f88\u591a\u9886\u57df\u7684\u4e8b\u5b9e\u89e3\u51b3\u65b9\u6848\uff0c\u5982NLP\u9886\u57df\u7684GPT-3\uff0cCV\u9886\u57df\u7684ViT\u3002 \u3000\u3000Uni-Mol\u662f\u6df1\u52bf\u79d1\u6280\u56e2\u961f\u53d1\u5e03\u76843D\u5206\u5b50\u9884\u8bad\u7ec3\u6a21\u578b\uff0cUni-Mol\u5c06\u5206\u5b503D\u7ed3\u6784\u4f5c\u4e3a\u6a21\u578b\u8f93\u5165\uff0c\u800c\u975e1D\u6216\u80052D\u4fe1\u606f\u3002\u4ece3D\u4fe1\u606f\u51fa\u53d1\u7684\u8868\u5f81\u5b66\u4e60\u8ba9Uni-Mol\u5728\u5173\u4e8e\u836f\u7269\u5206\u5b50\u3001\u86cb\u767d\u7ed3\u5408\u53e3\u888b\u7684\u4f17\u591a\u4e0b\u6e38\u4efb\u52a1\u8fbe\u5230\u4e86SOTA\u3002 \u8840\u8111\u5c4f\u969c\u900f\u8fc7\u80fd\u529b \u3000\u3000\u836f\u7269\u5206\u5b50\u80fd\u5426\u900f\u8fc7\u8840\u8111\u5c4f\u969c(blood brain barrior, BBB)\u662f\u836f\u7269\u7684\u91cd\u8981\u5c5e\u6027\uff0c\u5bf9\u4e8e\u4f5c\u7528\u4e8e\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u836f\u7269\uff0c\u836f\u7269\u5206\u5b50\u5fc5\u987b\u8981\u901a\u8fc7\u8840\u8111\u5c4f\u969c\u624d\u80fd\u5230\u8fbe\u4f5c\u7528\u9776\u70b9\uff0c\u800c\u5bf9\u4e8e\u90a3\u4e9b\u4f5c\u7528\u9776\u70b9\u4e0d\u5728\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u836f\u7269\uff0c\u8fd9\u4e9b\u836f\u7269\u5206\u5b50\u7684\u8840\u8111\u5c4f\u969c\u900f\u8fc7\u80fd\u529b\u8981\u5c3d\u53ef\u80fd\u7684\u4f4e\uff0c\u4ee5\u514d\u4ea7\u751f\u795e\u7ecf\u6bd2\u526f\u4f5c\u7528\u3002 \u9879\u76ee\u76ee\u6807 \u3000\u3000\u5229\u7528\u6df1\u52bf\u79d1\u6280\u56e2\u961f\u63d0\u4f9b\u7684Uni-Mol\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u5bf9\u5e94\u7684BBB\u6570\u636e\u96c6\u4e0a\u8fdb\u884cfinetune\uff0c\u8bc4\u6d4b\u6307\u6807\u662fAUC\u3002","title":"Welcome to UM-BBB"},{"location":"#welcome-to-um-bbb","text":"UM-BBB\u662f\u672c\u4eba\u5728Uni-Mol\u57fa\u7840\u4e0a\u7528\u4e8e\u9879\u76ee\u7ec3\u4e60\u7684\u5b9e\u8df5\u4ea7\u7269\uff0c\u5173\u4e8eUni-Mol\u7684\u8be6\u7ec6\u4fe1\u606f\u8bf7\u8bbf\u95ee uni-mol\u5b98\u65b9\u7f51\u5740 .","title":"Welcome to UM-BBB"},{"location":"#_1","text":"\u59d3\u540d \u2003\u6768\u5408 \u6559\u80b2\u80cc\u666f \u2003\u672c\u79d1 \u4e2d\u5c71\u5927\u5b66 \u836f\u5b66\u9662 \u836f\u5b66\u4e13\u4e1a \u2003\u7855\u58eb \u56fd\u79d1\u5927 \u6df1\u5733\u5148\u8fdb\u6280\u672f\u7814\u7a76\u9662 \u8ba1\u7b97\u751f\u7269\u5b66\u65b9\u5411","title":"\u4f5c\u8005\u4fe1\u606f"},{"location":"#_2","text":"Uni-Mol\u7b80\u4ecb \u3000\u3000\u4ece\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u6570\u636e\u4e2d\u63d0\u53d6\u8868\u5f81\u4fe1\u606f\uff0c\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6b63\u5728\u5e2d\u5377AI\u9886\u57df\uff0c\u5e76\u6210\u4e3a\u5f88\u591a\u9886\u57df\u7684\u4e8b\u5b9e\u89e3\u51b3\u65b9\u6848\uff0c\u5982NLP\u9886\u57df\u7684GPT-3\uff0cCV\u9886\u57df\u7684ViT\u3002 \u3000\u3000Uni-Mol\u662f\u6df1\u52bf\u79d1\u6280\u56e2\u961f\u53d1\u5e03\u76843D\u5206\u5b50\u9884\u8bad\u7ec3\u6a21\u578b\uff0cUni-Mol\u5c06\u5206\u5b503D\u7ed3\u6784\u4f5c\u4e3a\u6a21\u578b\u8f93\u5165\uff0c\u800c\u975e1D\u6216\u80052D\u4fe1\u606f\u3002\u4ece3D\u4fe1\u606f\u51fa\u53d1\u7684\u8868\u5f81\u5b66\u4e60\u8ba9Uni-Mol\u5728\u5173\u4e8e\u836f\u7269\u5206\u5b50\u3001\u86cb\u767d\u7ed3\u5408\u53e3\u888b\u7684\u4f17\u591a\u4e0b\u6e38\u4efb\u52a1\u8fbe\u5230\u4e86SOTA\u3002 \u8840\u8111\u5c4f\u969c\u900f\u8fc7\u80fd\u529b \u3000\u3000\u836f\u7269\u5206\u5b50\u80fd\u5426\u900f\u8fc7\u8840\u8111\u5c4f\u969c(blood brain barrior, BBB)\u662f\u836f\u7269\u7684\u91cd\u8981\u5c5e\u6027\uff0c\u5bf9\u4e8e\u4f5c\u7528\u4e8e\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u836f\u7269\uff0c\u836f\u7269\u5206\u5b50\u5fc5\u987b\u8981\u901a\u8fc7\u8840\u8111\u5c4f\u969c\u624d\u80fd\u5230\u8fbe\u4f5c\u7528\u9776\u70b9\uff0c\u800c\u5bf9\u4e8e\u90a3\u4e9b\u4f5c\u7528\u9776\u70b9\u4e0d\u5728\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u836f\u7269\uff0c\u8fd9\u4e9b\u836f\u7269\u5206\u5b50\u7684\u8840\u8111\u5c4f\u969c\u900f\u8fc7\u80fd\u529b\u8981\u5c3d\u53ef\u80fd\u7684\u4f4e\uff0c\u4ee5\u514d\u4ea7\u751f\u795e\u7ecf\u6bd2\u526f\u4f5c\u7528\u3002 \u9879\u76ee\u76ee\u6807 \u3000\u3000\u5229\u7528\u6df1\u52bf\u79d1\u6280\u56e2\u961f\u63d0\u4f9b\u7684Uni-Mol\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u5bf9\u5e94\u7684BBB\u6570\u636e\u96c6\u4e0a\u8fdb\u884cfinetune\uff0c\u8bc4\u6d4b\u6307\u6807\u662fAUC\u3002","title":"\u9879\u76ee\u80cc\u666f\u4e0e\u76ee\u6807"},{"location":"document/","text":"\u6280\u672f\u6587\u6863 \u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3Uni-Mol\uff0c\u6587\u6863\u7684\u524d\u534a\u90e8\u5206\u6839\u636e\u6587\u7ae0\u5185\u5bb9\uff0c\u4f7f\u7528\u516c\u5f0f\u548c\u603b\u7ed3\u6027\u8bed\u53e5\u63cf\u8ff0\u4e86Uni-Mol\u7684\u8bbe\u8ba1\u601d\u8def\uff0c\u6587\u6863\u7684\u540e\u534a\u90e8\u5206\u5219\u5c55\u793a\u4e86\u4e00\u4e9b\u5173\u952e\u516c\u5f0f\u548c\u6a21\u578b\u601d\u8def\u6240\u5bf9\u5e94\u7684\u4ee3\u7801 Uni-Mol\u6587\u7ae0\u5173\u4e8e\u6a21\u578b\u7684\u9610\u8ff0 \u4e0a\u56fe\u662fUni-Mol\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67b6\u6784\u56fe\uff0c\u53ef\u4ee5\u770b\u51faUni-Mol\u548c\u7ecf\u5178Transformer\u6a21\u578b\u7684\u5f02\u540c\u4e4b\u5904\u5728\u4e8e\uff1a \u8f93\u5165 \u7531\u4e8e\u672c\u8eab\u7684\u7f6e\u6362\u4e0d\u53d8\u6027\uff0cTransformer\u6a21\u578b\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u4f4d\u7f6e\u6ca1\u6709\u5206\u8fa8\u80fd\u529b\uff0c\u4e3a\u4e86\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u5316\u5408\u7269\u5206\u5b50\u76843D\u4fe1\u606f\uff0cUni-Mol\u5c06\u5750\u6807\u4fe1\u606f\u5d4c\u5165\u5230\u4fe1\u606f\u4f20\u64ad\u8fc7\u7a0b\uff0c\u6240\u4ee5Uni-Mol\u6709\u4e24\u4e2a\u8f93\u5165\uff0c\u5206\u522b\u662fatom types\u548catom coordinates\u3002 \u5728\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\uff0cUni-Mol\u548cTransformer\u90fd\u9700\u8981\u5bf9\u8f93\u5165\u8fdb\u884cmask\u64cd\u4f5c\uff0cUni-Mol\u5173\u4e8e\u5750\u6807\u7684mask\u4f7f\u7528\u4e86 noise range \u65b9\u6cd5\uff0c\u6240\u8c13 noise range \u662f\u6307\u5728\u539f\u59cb\u5750\u6807\u4e0a\u52a0\u4e00\u4e2a\u5904\u4e8e\u4e00\u5b9a\u8303\u56f4\u5185\u7684\u968f\u673a\u5750\u6807\u3002 \u6ce8\u610f\u529b\u5934 Uni-Mol\u6784\u5efa\u4e86\u4e09\u79cd\u7528\u4e8e\u8f93\u51fa\u7684head\uff0c\u5206\u522b\u5173\u6ce8\u6a21\u578b\u4e2d\u7684Atom Type\uff0cCoordinates\u4ee5\u53caPair-dist\u4fe1\u606f \u8868\u5f81(representation) \u4e3a\u4e86\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u5316\u5408\u7269\u7ed3\u6784\u4e2d\u539f\u5b50\u4fe1\u606f\u548c\u539f\u5b50\u95f4\u8ddd\u79bb\u4fe1\u606f\uff0cUni-Mol\u5728\u4fe1\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u7ef4\u6301\u4e86\u4e24\u79cd\u8868\u5f81\uff0c\u5206\u522b\u662fAtom Representation\u548cPair Representation\uff0c\u8fd9\u4e24\u79cd\u8868\u5f81\u5728\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u8fdb\u884c\u4fe1\u606f\u4ea4\u6362\u3002 \\[q_{ij}^{l+1}=q_{ij}^{l}+\\lbrace{\\frac{Q_{i}^{l,h} \\lbrace{K_{j}^{l,h}}\\rbrace^{T}}{\\sqrt{d}}}\\mid h \\in [1,H]\\rbrace \\tag{1}\\] \u516c\u5f0f1\u4ee3\u8868atom-to-pair\u7684\u4fe1\u606f\u4ea4\u6d41\uff0c \\(q_{ij}^{l}\\) \u662f\u539f\u5b50\u5bf9ij\u5728\u7b2cl\u5c42\u7684pair representation\uff0cH\u662f\u6ce8\u610f\u529b\u5934\u7684\u6570\u91cf\uff0cd\u662f\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \\(Q_{i}^{l,h}\\) ( \\(K_{j}^{l,h}\\) )\u662f\u7b2ci(j)\u4e2a\u539f\u5b50\u5173\u4e8e\u7b2cl\u5c42\u7b2ch\u4e2a\u6ce8\u610f\u529b\u5934\u7684query(key)\u3002 \\[Attention(Q_{i}^{l,h},K_{j}^{l,h},V_{j}^{l,h})=softmax({\\frac{Q_{i}^{l,h}\\lbrace{K_{i}^{l,h}}\\rbrace^{T}}{\\sqrt{d}}} + q_{ij}^{l-1,h})V_{j}^{l,h} \\tag{2}\\] \u516c\u5f0f2\u4ee3\u8868pair-to-atom\u7684\u4fe1\u606f\u4ea4\u6d41\uff0c \\(V_{i}^{l,h}\\) \u662f\u7b2cj\u4e2a\u539f\u5b50\u5728\u7b2cl\u5c42\u7b2ch\u4e2a\u6ce8\u610f\u529b\u5934\u7684value\u3002 \u9884\u6d4b3D\u5750\u6807 Uni-Mol\u5728\u6a21\u578b\u4e2d\u5f15\u5165\u5173\u6ce8\u539f\u5b50\u95f4\u8ddd\u79bb\u7684\u6ce8\u610f\u529b\u5934(Pair-dist Head)\uff0c\u901a\u8fc7\u8ba1\u7b97\u88ab\u63a9\u76d6\u539f\u5b50\u5750\u6807\u4e0e\u771f\u5b9e\u503c\u7684\u5dee\u5f02\uff0c\u5e76\u4e0e\u5750\u6807\u8f93\u5165\u503c\u76f8\u52a0\uff0c\u5f97\u5230\u539f\u5b50\u7684\u9884\u6d4b\u5750\u6807 \\[\\hat{x_i}=x_i+\\sum_{j = 0}^{n}\\frac{(x_i-x_j)C_{ij}}{n} \\tag{3}, c_{ij}=ReLU((q_{ij}^{L}-q_{ij}^{0})U)W\\] \u516c\u5f0f3\u4e2d\uff0cn\u4ee3\u8868\u5316\u5408\u7269\u4e2d\u7684\u539f\u5b50\u6570\u91cf\uff0cL\u662f\u6a21\u578b\u5c42\u6570\uff0c \\(x_i\\) \u662f\u7b2ci\u4e2a\u539f\u5b50\u7684\u8f93\u5165\u5750\u6807\uff0c \\(\\hat{x_i}\\) \u662f\u7b2ci\u4e2a\u539f\u5b50\u7684\u8f93\u51fa\u5750\u6807\u3002 Uni-Mol\u4ee3\u7801\u89e3\u8bfb unimol.py # Copyright (c) DP Technology. # This source code is licensed under the MIT license found in the # LICENSE file in the root directory of this source tree. import logging import torch import torch.nn as nn import torch.nn.functional as F from unicore import utils from unicore.models import BaseUnicoreModel, register_model, register_model_architecture from unicore.modules import LayerNorm, init_bert_params from .transformer_encoder_with_pair import TransformerEncoderWithPair from typing import Dict, Any, List logger = logging.getLogger(__name__) @register_model(\"unimol\") class UniMolModel(BaseUnicoreModel): @staticmethod def add_args(parser): \"\"\"Add model-specific arguments to the parser.\"\"\" pass def __init__(self, args, dictionary): super().__init__() base_architecture(args) self.args = args self.padding_idx = dictionary.pad() self.embed_tokens = nn.Embedding( len(dictionary), args.encoder_embed_dim, self.padding_idx ) self._num_updates = None self.encoder = TransformerEncoderWithPair( encoder_layers=args.encoder_layers, embed_dim=args.encoder_embed_dim, ffn_embed_dim=args.encoder_ffn_embed_dim, attention_heads=args.encoder_attention_heads, emb_dropout=args.emb_dropout, dropout=args.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.activation_dropout, max_seq_len=args.max_seq_len, activation_fn=args.activation_fn, no_final_head_layer_norm=args.delta_pair_repr_norm_loss < 0, ) if args.masked_token_loss > 0: self.lm_head = MaskLMHead( embed_dim=args.encoder_embed_dim, output_dim=len(dictionary), activation_fn=args.activation_fn, weight=None, ) K = 128 n_edge_type = len(dictionary) * len(dictionary) self.gbf_proj = NonLinearHead( K, args.encoder_attention_heads, args.activation_fn ) self.gbf = GaussianLayer(K, n_edge_type) if args.masked_coord_loss > 0: self.pair2coord_proj = NonLinearHead( args.encoder_attention_heads, 1, args.activation_fn ) if args.masked_dist_loss > 0: self.dist_head = DistanceHead( args.encoder_attention_heads, args.activation_fn ) self.classification_heads = nn.ModuleDict() self.apply(init_bert_params) @classmethod def build_model(cls, args, task): \"\"\"Build a new model instance.\"\"\" pass def forward( self, src_tokens, src_distance, src_coord, src_edge_type, encoder_masked_tokens=None, features_only=False, classification_head_name=None, **kwargs ): if classification_head_name is not None: features_only = True padding_mask = src_tokens.eq(self.padding_idx) if not padding_mask.any(): padding_mask = None # \u83b7\u53d6\u5173\u4e8eatom representation\u7684\u5d4c\u5165\u5411\u91cf x = self.embed_tokens(src_tokens) def get_dist_features(dist, et): n_node = dist.size(-1) # \u5c06\u8ddd\u79bb\u77e9\u9635\u7528\u9ad8\u65af\u6838\u51fd\u6570\u6620\u5c04\u5230\u591a\u7ef4\u77e9\u9635 gbf_feature = self.gbf(dist, et) gbf_result = self.gbf_proj(gbf_feature) graph_attn_bias = gbf_result graph_attn_bias = graph_attn_bias.permute(0, 3, 1, 2).contiguous() graph_attn_bias = graph_attn_bias.view(-1, n_node, n_node) return graph_attn_bias #\u83b7\u53d6\u5173\u4e8epair representation\u7684\u5d4c\u5165\u5411\u91cf graph_attn_bias = get_dist_features(src_distance, src_edge_type) ( encoder_rep, encoder_pair_rep, delta_encoder_pair_rep, x_norm, delta_encoder_pair_rep_norm, ) = self.encoder(x, padding_mask=padding_mask, attn_mask=graph_attn_bias) encoder_pair_rep[encoder_pair_rep == float(\"-inf\")] = 0 encoder_distance = None encoder_coord = None # \u9884\u8bad\u7ec3\u4e2d\u5bf9\u5e94\u7684\u4e09\u4e2a\u8f93\u51fa\u5934\uff0c\u5206\u522b\u5bf9\u5e94\u539f\u5b50\u3001\u5750\u6807\u548c\u8ddd\u79bb\u7684\u4fe1\u606f if not features_only: if self.args.masked_token_loss > 0: logits = self.lm_head(encoder_rep, encoder_masked_tokens) if self.args.masked_coord_loss > 0: coords_emb = src_coord if padding_mask is not None: atom_num = (torch.sum(1 - padding_mask.type_as(x), dim=1) - 1).view( -1, 1, 1, 1 ) else: atom_num = src_coord.shape[1] - 1 # \u516c\u5f0f3\u5bf9\u5e94\u7684\u4ee3\u7801 delta_pos = coords_emb.unsqueeze(1) - coords_emb.unsqueeze(2) attn_probs = self.pair2coord_proj(delta_encoder_pair_rep) coord_update = delta_pos / atom_num * attn_probs coord_update = torch.sum(coord_update, dim=2) encoder_coord = coords_emb + coord_update if self.args.masked_dist_loss > 0: encoder_distance = self.dist_head(encoder_pair_rep) # \u5fae\u8c03\u8bad\u7ec3\u4e2d\u7528\u4e8e\u5206\u7c7b\u7684head if classification_head_name is not None: logits = self.classification_heads[classification_head_name](encoder_rep) if self.args.mode == 'infer': return encoder_rep, encoder_pair_rep else: return ( logits, encoder_distance, encoder_coord, x_norm, delta_encoder_pair_rep_norm, ) def register_classification_head( self, name, num_classes=None, inner_dim=None, **kwargs ): \"\"\"Register a classification head.\"\"\" pass def set_num_updates(self, num_updates): pass def get_num_updates(self): pass class MaskLMHead(nn.Module): \"\"\"Head for masked language modeling.\"\"\" def __init__(self, embed_dim, output_dim, activation_fn, weight=None): super().__init__() self.dense = nn.Linear(embed_dim, embed_dim) self.activation_fn = utils.get_activation_fn(activation_fn) self.layer_norm = LayerNorm(embed_dim) if weight is None: weight = nn.Linear(embed_dim, output_dim, bias=False).weight self.weight = weight self.bias = nn.Parameter(torch.zeros(output_dim)) def forward(self, features, masked_tokens=None, **kwargs): # Only project the masked tokens while training, # saves both memory and computation if masked_tokens is not None: features = features[masked_tokens, :] x = self.dense(features) x = self.activation_fn(x) x = self.layer_norm(x) # project back to size of vocabulary with bias x = F.linear(x, self.weight) + self.bias return x class ClassificationHead(nn.Module): \"\"\"Head for sentence-level classification tasks.\"\"\" def __init__( self, input_dim, inner_dim, num_classes, activation_fn, pooler_dropout, ): super().__init__() self.dense = nn.Linear(input_dim, inner_dim) self.activation_fn = utils.get_activation_fn(activation_fn) self.dropout = nn.Dropout(p=pooler_dropout) self.out_proj = nn.Linear(inner_dim, num_classes) def forward(self, features, **kwargs): x = features[:, 0, :] # take <s> token (equiv. to [CLS]) x = self.dropout(x) x = self.dense(x) x = self.activation_fn(x) x = self.dropout(x) x = self.out_proj(x) return x class NonLinearHead(nn.Module): \"\"\"Head for simple classification tasks.\"\"\" def __init__( self, input_dim, out_dim, activation_fn, hidden=None, ): super().__init__() hidden = input_dim if not hidden else hidden self.linear1 = nn.Linear(input_dim, hidden) self.linear2 = nn.Linear(hidden, out_dim) self.activation_fn = utils.get_activation_fn(activation_fn) def forward(self, x): x = self.linear1(x) x = self.activation_fn(x) x = self.linear2(x) return x class DistanceHead(nn.Module): def __init__( self, heads, activation_fn, ): super().__init__() self.dense = nn.Linear(heads, heads) self.layer_norm = nn.LayerNorm(heads) self.out_proj = nn.Linear(heads, 1) self.activation_fn = utils.get_activation_fn(activation_fn) def forward(self, x): bsz, seq_len, seq_len, _ = x.size() # x[x == float('-inf')] = 0 x = self.dense(x) x = self.activation_fn(x) x = self.layer_norm(x) x = self.out_proj(x).view(bsz, seq_len, seq_len) x = (x + x.transpose(-1, -2)) * 0.5 return x @torch.jit.script def gaussian(x, mean, std): pi = 3.14159 a = (2 * pi) ** 0.5 return torch.exp(-0.5 * (((x - mean) / std) ** 2)) / (a * std) class GaussianLayer(nn.Module): def __init__(self, K=128, edge_types=1024): super().__init__() self.K = K self.means = nn.Embedding(1, K) self.stds = nn.Embedding(1, K) self.mul = nn.Embedding(edge_types, 1) self.bias = nn.Embedding(edge_types, 1) nn.init.uniform_(self.means.weight, 0, 3) nn.init.uniform_(self.stds.weight, 0, 3) nn.init.constant_(self.bias.weight, 0) nn.init.constant_(self.mul.weight, 1) def forward(self, x, edge_type): mul = self.mul(edge_type).type_as(x) bias = self.bias(edge_type).type_as(x) x = mul * x.unsqueeze(-1) + bias x = x.expand(-1, -1, -1, self.K) mean = self.means.weight.float().view(-1) std = self.stds.weight.float().view(-1).abs() + 1e-5 return gaussian(x.float(), mean, std).type_as(self.means.weight) @register_model_architecture(\"unimol\", \"unimol\") def base_architecture(args): args.encoder_layers = getattr(args, \"encoder_layers\", 15) args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 512) args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 2048) args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 64) args.dropout = getattr(args, \"dropout\", 0.1) args.emb_dropout = getattr(args, \"emb_dropout\", 0.1) args.attention_dropout = getattr(args, \"attention_dropout\", 0.1) args.activation_dropout = getattr(args, \"activation_dropout\", 0.0) args.pooler_dropout = getattr(args, \"pooler_dropout\", 0.0) args.max_seq_len = getattr(args, \"max_seq_len\", 512) args.activation_fn = getattr(args, \"activation_fn\", \"gelu\") args.pooler_activation_fn = getattr(args, \"pooler_activation_fn\", \"tanh\") args.post_ln = getattr(args, \"post_ln\", False) args.masked_token_loss = getattr(args, \"masked_token_loss\", -1.0) args.masked_coord_loss = getattr(args, \"masked_coord_loss\", -1.0) args.masked_dist_loss = getattr(args, \"masked_dist_loss\", -1.0) args.x_norm_loss = getattr(args, \"x_norm_loss\", -1.0) args.delta_pair_repr_norm_loss = getattr(args, \"delta_pair_repr_norm_loss\", -1.0) @register_model_architecture(\"unimol\", \"unimol_base\") def unimol_base_architecture(args): base_architecture(args) transformer_encoder_with_pair.py # Copyright (c) DP Technology. # This source code is licensed under the MIT license found in the # LICENSE file in the root directory of this source tree. from typing import Optional import math import torch import torch.nn as nn import torch.nn.functional as F from unicore.modules import TransformerEncoderLayer, LayerNorm class TransformerEncoderWithPair(nn.Module): def __init__( self, encoder_layers: int = 6, embed_dim: int = 768, ffn_embed_dim: int = 3072, attention_heads: int = 8, emb_dropout: float = 0.1, dropout: float = 0.1, attention_dropout: float = 0.1, activation_dropout: float = 0.0, max_seq_len: int = 256, activation_fn: str = \"gelu\", post_ln: bool = False, no_final_head_layer_norm: bool = False, ) -> None: super().__init__() self.emb_dropout = emb_dropout self.max_seq_len = max_seq_len self.embed_dim = embed_dim self.attention_heads = attention_heads self.emb_layer_norm = LayerNorm(self.embed_dim) if not post_ln: self.final_layer_norm = LayerNorm(self.embed_dim) else: self.final_layer_norm = None if not no_final_head_layer_norm: self.final_head_layer_norm = LayerNorm(attention_heads) else: self.final_head_layer_norm = None self.layers = nn.ModuleList( [ TransformerEncoderLayer( embed_dim=self.embed_dim, ffn_embed_dim=ffn_embed_dim, attention_heads=attention_heads, dropout=dropout, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn, post_ln=post_ln, ) for _ in range(encoder_layers) ] ) def forward( self, emb: torch.Tensor, attn_mask: Optional[torch.Tensor] = None, padding_mask: Optional[torch.Tensor] = None, ) -> torch.Tensor: bsz = emb.size(0) seq_len = emb.size(1) x = self.emb_layer_norm(emb) x = F.dropout(x, p=self.emb_dropout, training=self.training) # account for padding while computing the representation if padding_mask is not None: # \u9884\u8bad\u7ec3\u4e2d\uff0c\u83b7\u53d6\u63a9\u76d6\u4e4b\u540e\u7684atom representation x = x * (1 - padding_mask.unsqueeze(-1).type_as(x)) input_attn_mask = attn_mask input_padding_mask = padding_mask def fill_attn_mask(attn_mask, padding_mask, fill_val=float(\"-inf\")): if attn_mask is not None and padding_mask is not None: # merge key_padding_mask and attn_mask attn_mask = attn_mask.view(x.size(0), -1, seq_len, seq_len) attn_mask.masked_fill_( padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), fill_val, ) attn_mask = attn_mask.view(-1, seq_len, seq_len) padding_mask = None return attn_mask, padding_mask assert attn_mask is not None # \u83b7\u53d6\u63a9\u76d6\u4e4b\u540e\u7684pair representation attn_mask, padding_mask = fill_attn_mask(attn_mask, padding_mask) # \u4fe1\u606f\u5728\u5c42\u95f4\u4f20\u64ad\u8fc7\u7a0b\u4e2d\uff0c\u7ef4\u6301atom representation\u548cpair representation\u7684\u5b58\u5728 for i in range(len(self.layers)): x, attn_mask, _ = self.layers[i]( x, padding_mask=padding_mask, attn_bias=attn_mask, return_attn=True ) def norm_loss(x, eps=1e-10, tolerance=1.0): x = x.float() max_norm = x.shape[-1] ** 0.5 norm = torch.sqrt(torch.sum(x**2, dim=-1) + eps) error = torch.nn.functional.relu((norm - max_norm).abs() - tolerance) return error def masked_mean(mask, value, dim=-1, eps=1e-10): return ( torch.sum(mask * value, dim=dim) / (eps + torch.sum(mask, dim=dim)) ).mean() x_norm = norm_loss(x) if input_padding_mask is not None: token_mask = 1.0 - input_padding_mask.float() else: token_mask = torch.ones_like(x_norm, device=x_norm.device) x_norm = masked_mean(token_mask, x_norm) if self.final_layer_norm is not None: x = self.final_layer_norm(x) delta_pair_repr = attn_mask - input_attn_mask delta_pair_repr, _ = fill_attn_mask(delta_pair_repr, input_padding_mask, 0) attn_mask = ( attn_mask.view(bsz, -1, seq_len, seq_len).permute(0, 2, 3, 1).contiguous() ) delta_pair_repr = ( delta_pair_repr.view(bsz, -1, seq_len, seq_len) .permute(0, 2, 3, 1) .contiguous() ) pair_mask = token_mask[..., None] * token_mask[..., None, :] delta_pair_repr_norm = norm_loss(delta_pair_repr) delta_pair_repr_norm = masked_mean( pair_mask, delta_pair_repr_norm, dim=(-1, -2) ) if self.final_head_layer_norm is not None: delta_pair_repr = self.final_head_layer_norm(delta_pair_repr) return x, attn_mask, delta_pair_repr, x_norm, delta_pair_repr_norm transformer_encoder_layer.py from typing import Dict, Optional import torch import torch.nn.functional as F from unicore import utils from torch import nn from . import LayerNorm, SelfMultiheadAttention class TransformerEncoderLayer(nn.Module): \"\"\" Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained models. \"\"\" def __init__( self, embed_dim: int = 768, ffn_embed_dim: int = 3072, attention_heads: int = 8, dropout: float = 0.1, attention_dropout: float = 0.1, activation_dropout: float = 0.0, activation_fn: str = \"gelu\", post_ln = False, ) -> None: super().__init__() # Initialize parameters self.embed_dim = embed_dim self.attention_heads = attention_heads self.attention_dropout = attention_dropout self.dropout = dropout self.activation_dropout = activation_dropout self.activation_fn = utils.get_activation_fn(activation_fn) self.self_attn = SelfMultiheadAttention( self.embed_dim, attention_heads, dropout=attention_dropout, ) # layer norm associated with the self attention layer self.self_attn_layer_norm = LayerNorm(self.embed_dim) self.fc1 = nn.Linear(self.embed_dim, ffn_embed_dim) self.fc2 = nn.Linear(ffn_embed_dim, self.embed_dim) self.final_layer_norm = LayerNorm(self.embed_dim) self.post_ln = post_ln def forward( self, x: torch.Tensor, attn_bias: Optional[torch.Tensor] = None, padding_mask: Optional[torch.Tensor] = None, return_attn: bool=False, ) -> torch.Tensor: \"\"\" LayerNorm is applied either before or after the self-attention/ffn modules similar to the original Transformer implementation. \"\"\" residual = x # \u5c42\u6b63\u5219\u5316\u7684\u4f5c\u7528 if not self.post_ln: x = self.self_attn_layer_norm(x) x = self.self_attn( query=x, key_padding_mask=padding_mask, attn_bias=attn_bias, return_attn=return_attn, ) if return_attn: # \u5982\u679creturn_attn==True\uff0c\u6b64\u65f6\u7684x = [o, attn_weights, attn] x, attn_weights, attn_probs = x x = F.dropout(x, p=self.dropout, training=self.training) x = residual + x if self.post_ln: x = self.self_attn_layer_norm(x) residual = x if not self.post_ln: x = self.final_layer_norm(x) x = self.fc1(x) x = self.activation_fn(x) x = F.dropout(x, p=self.activation_dropout, training=self.training) x = self.fc2(x) x = F.dropout(x, p=self.dropout, training=self.training) x = residual + x if self.post_ln: x = self.final_layer_norm(x) if not return_attn: return x else: return x, attn_weights, attn_probs multihead_attention.py class SelfMultiheadAttention(nn.Module): def __init__( self, embed_dim, num_heads, dropout=0.1, bias=True, scaling_factor=1, ): super().__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.dropout = dropout self.head_dim = embed_dim // num_heads assert ( self.head_dim * num_heads == self.embed_dim ), \"embed_dim must be divisible by num_heads\" self.scaling = (self.head_dim * scaling_factor) ** -0.5 self.in_proj = nn.Linear(embed_dim, embed_dim * 3, bias=bias) self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias) def forward( self, query, key_padding_mask: Optional[Tensor] = None, attn_bias: Optional[Tensor] = None, return_attn: bool = False, ) -> Tensor: bsz, tgt_len, embed_dim = query.size() assert embed_dim == self.embed_dim q, k, v = self.in_proj(query).chunk(3, dim=-1) q = ( q.view(bsz, tgt_len, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) * self.scaling ) if k is not None: k = ( k.view(bsz, -1, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) ) if v is not None: v = ( v.view(bsz, -1, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) ) assert k is not None src_len = k.size(1) # This is part of a workaround to get around fork/join parallelism # not supporting Optional types. if key_padding_mask is not None and key_padding_mask.dim() == 0: key_padding_mask = None if key_padding_mask is not None: assert key_padding_mask.size(0) == bsz assert key_padding_mask.size(1) == src_len attn_weights = torch.bmm(q, k.transpose(1, 2)) assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len] if key_padding_mask is not None: # don't attend to padding symbols attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) attn_weights.masked_fill_( key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float(\"-inf\") ) attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len) if not return_attn: attn = softmax_dropout( attn_weights, self.dropout, self.training, bias=attn_bias, ) else: # \u516c\u5f0f2\u5bf9\u5e94\u7684\u4ee3\u7801 attn_weights += attn_bias attn = softmax_dropout( attn_weights, self.dropout, self.training, inplace=False, ) o = torch.bmm(attn, v) assert list(o.size()) == [bsz * self.num_heads, tgt_len, self.head_dim] o = ( o.view(bsz, self.num_heads, tgt_len, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz, tgt_len, embed_dim) ) o = self.out_proj(o) if not return_attn: return o else: # attn_weights\u4ee3\u8868\u539f\u5b50\u95f4\u7684attention\u77e9\u9635 # \u8f93\u51fa\u540e\u4f5c\u4e3a\u4e0b\u4e00\u5c42\u7684pair repretention # \u5bf9\u5e94\u4e8e\u516c\u5f0f1 return o, attn_weights, attn","title":"\u6280\u672f\u6587\u6863"},{"location":"document/#_1","text":"\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3Uni-Mol\uff0c\u6587\u6863\u7684\u524d\u534a\u90e8\u5206\u6839\u636e\u6587\u7ae0\u5185\u5bb9\uff0c\u4f7f\u7528\u516c\u5f0f\u548c\u603b\u7ed3\u6027\u8bed\u53e5\u63cf\u8ff0\u4e86Uni-Mol\u7684\u8bbe\u8ba1\u601d\u8def\uff0c\u6587\u6863\u7684\u540e\u534a\u90e8\u5206\u5219\u5c55\u793a\u4e86\u4e00\u4e9b\u5173\u952e\u516c\u5f0f\u548c\u6a21\u578b\u601d\u8def\u6240\u5bf9\u5e94\u7684\u4ee3\u7801","title":"\u6280\u672f\u6587\u6863"},{"location":"document/#uni-mol","text":"\u4e0a\u56fe\u662fUni-Mol\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67b6\u6784\u56fe\uff0c\u53ef\u4ee5\u770b\u51faUni-Mol\u548c\u7ecf\u5178Transformer\u6a21\u578b\u7684\u5f02\u540c\u4e4b\u5904\u5728\u4e8e\uff1a","title":"Uni-Mol\u6587\u7ae0\u5173\u4e8e\u6a21\u578b\u7684\u9610\u8ff0"},{"location":"document/#_2","text":"\u7531\u4e8e\u672c\u8eab\u7684\u7f6e\u6362\u4e0d\u53d8\u6027\uff0cTransformer\u6a21\u578b\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u4f4d\u7f6e\u6ca1\u6709\u5206\u8fa8\u80fd\u529b\uff0c\u4e3a\u4e86\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u5316\u5408\u7269\u5206\u5b50\u76843D\u4fe1\u606f\uff0cUni-Mol\u5c06\u5750\u6807\u4fe1\u606f\u5d4c\u5165\u5230\u4fe1\u606f\u4f20\u64ad\u8fc7\u7a0b\uff0c\u6240\u4ee5Uni-Mol\u6709\u4e24\u4e2a\u8f93\u5165\uff0c\u5206\u522b\u662fatom types\u548catom coordinates\u3002 \u5728\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\uff0cUni-Mol\u548cTransformer\u90fd\u9700\u8981\u5bf9\u8f93\u5165\u8fdb\u884cmask\u64cd\u4f5c\uff0cUni-Mol\u5173\u4e8e\u5750\u6807\u7684mask\u4f7f\u7528\u4e86 noise range \u65b9\u6cd5\uff0c\u6240\u8c13 noise range \u662f\u6307\u5728\u539f\u59cb\u5750\u6807\u4e0a\u52a0\u4e00\u4e2a\u5904\u4e8e\u4e00\u5b9a\u8303\u56f4\u5185\u7684\u968f\u673a\u5750\u6807\u3002","title":"\u8f93\u5165"},{"location":"document/#_3","text":"Uni-Mol\u6784\u5efa\u4e86\u4e09\u79cd\u7528\u4e8e\u8f93\u51fa\u7684head\uff0c\u5206\u522b\u5173\u6ce8\u6a21\u578b\u4e2d\u7684Atom Type\uff0cCoordinates\u4ee5\u53caPair-dist\u4fe1\u606f","title":"\u6ce8\u610f\u529b\u5934"},{"location":"document/#representation","text":"\u4e3a\u4e86\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u5316\u5408\u7269\u7ed3\u6784\u4e2d\u539f\u5b50\u4fe1\u606f\u548c\u539f\u5b50\u95f4\u8ddd\u79bb\u4fe1\u606f\uff0cUni-Mol\u5728\u4fe1\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u7ef4\u6301\u4e86\u4e24\u79cd\u8868\u5f81\uff0c\u5206\u522b\u662fAtom Representation\u548cPair Representation\uff0c\u8fd9\u4e24\u79cd\u8868\u5f81\u5728\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u8fdb\u884c\u4fe1\u606f\u4ea4\u6362\u3002 \\[q_{ij}^{l+1}=q_{ij}^{l}+\\lbrace{\\frac{Q_{i}^{l,h} \\lbrace{K_{j}^{l,h}}\\rbrace^{T}}{\\sqrt{d}}}\\mid h \\in [1,H]\\rbrace \\tag{1}\\] \u516c\u5f0f1\u4ee3\u8868atom-to-pair\u7684\u4fe1\u606f\u4ea4\u6d41\uff0c \\(q_{ij}^{l}\\) \u662f\u539f\u5b50\u5bf9ij\u5728\u7b2cl\u5c42\u7684pair representation\uff0cH\u662f\u6ce8\u610f\u529b\u5934\u7684\u6570\u91cf\uff0cd\u662f\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \\(Q_{i}^{l,h}\\) ( \\(K_{j}^{l,h}\\) )\u662f\u7b2ci(j)\u4e2a\u539f\u5b50\u5173\u4e8e\u7b2cl\u5c42\u7b2ch\u4e2a\u6ce8\u610f\u529b\u5934\u7684query(key)\u3002 \\[Attention(Q_{i}^{l,h},K_{j}^{l,h},V_{j}^{l,h})=softmax({\\frac{Q_{i}^{l,h}\\lbrace{K_{i}^{l,h}}\\rbrace^{T}}{\\sqrt{d}}} + q_{ij}^{l-1,h})V_{j}^{l,h} \\tag{2}\\] \u516c\u5f0f2\u4ee3\u8868pair-to-atom\u7684\u4fe1\u606f\u4ea4\u6d41\uff0c \\(V_{i}^{l,h}\\) \u662f\u7b2cj\u4e2a\u539f\u5b50\u5728\u7b2cl\u5c42\u7b2ch\u4e2a\u6ce8\u610f\u529b\u5934\u7684value\u3002","title":"\u8868\u5f81(representation)"},{"location":"document/#3d","text":"Uni-Mol\u5728\u6a21\u578b\u4e2d\u5f15\u5165\u5173\u6ce8\u539f\u5b50\u95f4\u8ddd\u79bb\u7684\u6ce8\u610f\u529b\u5934(Pair-dist Head)\uff0c\u901a\u8fc7\u8ba1\u7b97\u88ab\u63a9\u76d6\u539f\u5b50\u5750\u6807\u4e0e\u771f\u5b9e\u503c\u7684\u5dee\u5f02\uff0c\u5e76\u4e0e\u5750\u6807\u8f93\u5165\u503c\u76f8\u52a0\uff0c\u5f97\u5230\u539f\u5b50\u7684\u9884\u6d4b\u5750\u6807 \\[\\hat{x_i}=x_i+\\sum_{j = 0}^{n}\\frac{(x_i-x_j)C_{ij}}{n} \\tag{3}, c_{ij}=ReLU((q_{ij}^{L}-q_{ij}^{0})U)W\\] \u516c\u5f0f3\u4e2d\uff0cn\u4ee3\u8868\u5316\u5408\u7269\u4e2d\u7684\u539f\u5b50\u6570\u91cf\uff0cL\u662f\u6a21\u578b\u5c42\u6570\uff0c \\(x_i\\) \u662f\u7b2ci\u4e2a\u539f\u5b50\u7684\u8f93\u5165\u5750\u6807\uff0c \\(\\hat{x_i}\\) \u662f\u7b2ci\u4e2a\u539f\u5b50\u7684\u8f93\u51fa\u5750\u6807\u3002","title":"\u9884\u6d4b3D\u5750\u6807"},{"location":"document/#uni-mol_1","text":"","title":"Uni-Mol\u4ee3\u7801\u89e3\u8bfb"},{"location":"document/#unimolpy","text":"# Copyright (c) DP Technology. # This source code is licensed under the MIT license found in the # LICENSE file in the root directory of this source tree. import logging import torch import torch.nn as nn import torch.nn.functional as F from unicore import utils from unicore.models import BaseUnicoreModel, register_model, register_model_architecture from unicore.modules import LayerNorm, init_bert_params from .transformer_encoder_with_pair import TransformerEncoderWithPair from typing import Dict, Any, List logger = logging.getLogger(__name__) @register_model(\"unimol\") class UniMolModel(BaseUnicoreModel): @staticmethod def add_args(parser): \"\"\"Add model-specific arguments to the parser.\"\"\" pass def __init__(self, args, dictionary): super().__init__() base_architecture(args) self.args = args self.padding_idx = dictionary.pad() self.embed_tokens = nn.Embedding( len(dictionary), args.encoder_embed_dim, self.padding_idx ) self._num_updates = None self.encoder = TransformerEncoderWithPair( encoder_layers=args.encoder_layers, embed_dim=args.encoder_embed_dim, ffn_embed_dim=args.encoder_ffn_embed_dim, attention_heads=args.encoder_attention_heads, emb_dropout=args.emb_dropout, dropout=args.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.activation_dropout, max_seq_len=args.max_seq_len, activation_fn=args.activation_fn, no_final_head_layer_norm=args.delta_pair_repr_norm_loss < 0, ) if args.masked_token_loss > 0: self.lm_head = MaskLMHead( embed_dim=args.encoder_embed_dim, output_dim=len(dictionary), activation_fn=args.activation_fn, weight=None, ) K = 128 n_edge_type = len(dictionary) * len(dictionary) self.gbf_proj = NonLinearHead( K, args.encoder_attention_heads, args.activation_fn ) self.gbf = GaussianLayer(K, n_edge_type) if args.masked_coord_loss > 0: self.pair2coord_proj = NonLinearHead( args.encoder_attention_heads, 1, args.activation_fn ) if args.masked_dist_loss > 0: self.dist_head = DistanceHead( args.encoder_attention_heads, args.activation_fn ) self.classification_heads = nn.ModuleDict() self.apply(init_bert_params) @classmethod def build_model(cls, args, task): \"\"\"Build a new model instance.\"\"\" pass def forward( self, src_tokens, src_distance, src_coord, src_edge_type, encoder_masked_tokens=None, features_only=False, classification_head_name=None, **kwargs ): if classification_head_name is not None: features_only = True padding_mask = src_tokens.eq(self.padding_idx) if not padding_mask.any(): padding_mask = None # \u83b7\u53d6\u5173\u4e8eatom representation\u7684\u5d4c\u5165\u5411\u91cf x = self.embed_tokens(src_tokens) def get_dist_features(dist, et): n_node = dist.size(-1) # \u5c06\u8ddd\u79bb\u77e9\u9635\u7528\u9ad8\u65af\u6838\u51fd\u6570\u6620\u5c04\u5230\u591a\u7ef4\u77e9\u9635 gbf_feature = self.gbf(dist, et) gbf_result = self.gbf_proj(gbf_feature) graph_attn_bias = gbf_result graph_attn_bias = graph_attn_bias.permute(0, 3, 1, 2).contiguous() graph_attn_bias = graph_attn_bias.view(-1, n_node, n_node) return graph_attn_bias #\u83b7\u53d6\u5173\u4e8epair representation\u7684\u5d4c\u5165\u5411\u91cf graph_attn_bias = get_dist_features(src_distance, src_edge_type) ( encoder_rep, encoder_pair_rep, delta_encoder_pair_rep, x_norm, delta_encoder_pair_rep_norm, ) = self.encoder(x, padding_mask=padding_mask, attn_mask=graph_attn_bias) encoder_pair_rep[encoder_pair_rep == float(\"-inf\")] = 0 encoder_distance = None encoder_coord = None # \u9884\u8bad\u7ec3\u4e2d\u5bf9\u5e94\u7684\u4e09\u4e2a\u8f93\u51fa\u5934\uff0c\u5206\u522b\u5bf9\u5e94\u539f\u5b50\u3001\u5750\u6807\u548c\u8ddd\u79bb\u7684\u4fe1\u606f if not features_only: if self.args.masked_token_loss > 0: logits = self.lm_head(encoder_rep, encoder_masked_tokens) if self.args.masked_coord_loss > 0: coords_emb = src_coord if padding_mask is not None: atom_num = (torch.sum(1 - padding_mask.type_as(x), dim=1) - 1).view( -1, 1, 1, 1 ) else: atom_num = src_coord.shape[1] - 1 # \u516c\u5f0f3\u5bf9\u5e94\u7684\u4ee3\u7801 delta_pos = coords_emb.unsqueeze(1) - coords_emb.unsqueeze(2) attn_probs = self.pair2coord_proj(delta_encoder_pair_rep) coord_update = delta_pos / atom_num * attn_probs coord_update = torch.sum(coord_update, dim=2) encoder_coord = coords_emb + coord_update if self.args.masked_dist_loss > 0: encoder_distance = self.dist_head(encoder_pair_rep) # \u5fae\u8c03\u8bad\u7ec3\u4e2d\u7528\u4e8e\u5206\u7c7b\u7684head if classification_head_name is not None: logits = self.classification_heads[classification_head_name](encoder_rep) if self.args.mode == 'infer': return encoder_rep, encoder_pair_rep else: return ( logits, encoder_distance, encoder_coord, x_norm, delta_encoder_pair_rep_norm, ) def register_classification_head( self, name, num_classes=None, inner_dim=None, **kwargs ): \"\"\"Register a classification head.\"\"\" pass def set_num_updates(self, num_updates): pass def get_num_updates(self): pass class MaskLMHead(nn.Module): \"\"\"Head for masked language modeling.\"\"\" def __init__(self, embed_dim, output_dim, activation_fn, weight=None): super().__init__() self.dense = nn.Linear(embed_dim, embed_dim) self.activation_fn = utils.get_activation_fn(activation_fn) self.layer_norm = LayerNorm(embed_dim) if weight is None: weight = nn.Linear(embed_dim, output_dim, bias=False).weight self.weight = weight self.bias = nn.Parameter(torch.zeros(output_dim)) def forward(self, features, masked_tokens=None, **kwargs): # Only project the masked tokens while training, # saves both memory and computation if masked_tokens is not None: features = features[masked_tokens, :] x = self.dense(features) x = self.activation_fn(x) x = self.layer_norm(x) # project back to size of vocabulary with bias x = F.linear(x, self.weight) + self.bias return x class ClassificationHead(nn.Module): \"\"\"Head for sentence-level classification tasks.\"\"\" def __init__( self, input_dim, inner_dim, num_classes, activation_fn, pooler_dropout, ): super().__init__() self.dense = nn.Linear(input_dim, inner_dim) self.activation_fn = utils.get_activation_fn(activation_fn) self.dropout = nn.Dropout(p=pooler_dropout) self.out_proj = nn.Linear(inner_dim, num_classes) def forward(self, features, **kwargs): x = features[:, 0, :] # take <s> token (equiv. to [CLS]) x = self.dropout(x) x = self.dense(x) x = self.activation_fn(x) x = self.dropout(x) x = self.out_proj(x) return x class NonLinearHead(nn.Module): \"\"\"Head for simple classification tasks.\"\"\" def __init__( self, input_dim, out_dim, activation_fn, hidden=None, ): super().__init__() hidden = input_dim if not hidden else hidden self.linear1 = nn.Linear(input_dim, hidden) self.linear2 = nn.Linear(hidden, out_dim) self.activation_fn = utils.get_activation_fn(activation_fn) def forward(self, x): x = self.linear1(x) x = self.activation_fn(x) x = self.linear2(x) return x class DistanceHead(nn.Module): def __init__( self, heads, activation_fn, ): super().__init__() self.dense = nn.Linear(heads, heads) self.layer_norm = nn.LayerNorm(heads) self.out_proj = nn.Linear(heads, 1) self.activation_fn = utils.get_activation_fn(activation_fn) def forward(self, x): bsz, seq_len, seq_len, _ = x.size() # x[x == float('-inf')] = 0 x = self.dense(x) x = self.activation_fn(x) x = self.layer_norm(x) x = self.out_proj(x).view(bsz, seq_len, seq_len) x = (x + x.transpose(-1, -2)) * 0.5 return x @torch.jit.script def gaussian(x, mean, std): pi = 3.14159 a = (2 * pi) ** 0.5 return torch.exp(-0.5 * (((x - mean) / std) ** 2)) / (a * std) class GaussianLayer(nn.Module): def __init__(self, K=128, edge_types=1024): super().__init__() self.K = K self.means = nn.Embedding(1, K) self.stds = nn.Embedding(1, K) self.mul = nn.Embedding(edge_types, 1) self.bias = nn.Embedding(edge_types, 1) nn.init.uniform_(self.means.weight, 0, 3) nn.init.uniform_(self.stds.weight, 0, 3) nn.init.constant_(self.bias.weight, 0) nn.init.constant_(self.mul.weight, 1) def forward(self, x, edge_type): mul = self.mul(edge_type).type_as(x) bias = self.bias(edge_type).type_as(x) x = mul * x.unsqueeze(-1) + bias x = x.expand(-1, -1, -1, self.K) mean = self.means.weight.float().view(-1) std = self.stds.weight.float().view(-1).abs() + 1e-5 return gaussian(x.float(), mean, std).type_as(self.means.weight) @register_model_architecture(\"unimol\", \"unimol\") def base_architecture(args): args.encoder_layers = getattr(args, \"encoder_layers\", 15) args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 512) args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 2048) args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 64) args.dropout = getattr(args, \"dropout\", 0.1) args.emb_dropout = getattr(args, \"emb_dropout\", 0.1) args.attention_dropout = getattr(args, \"attention_dropout\", 0.1) args.activation_dropout = getattr(args, \"activation_dropout\", 0.0) args.pooler_dropout = getattr(args, \"pooler_dropout\", 0.0) args.max_seq_len = getattr(args, \"max_seq_len\", 512) args.activation_fn = getattr(args, \"activation_fn\", \"gelu\") args.pooler_activation_fn = getattr(args, \"pooler_activation_fn\", \"tanh\") args.post_ln = getattr(args, \"post_ln\", False) args.masked_token_loss = getattr(args, \"masked_token_loss\", -1.0) args.masked_coord_loss = getattr(args, \"masked_coord_loss\", -1.0) args.masked_dist_loss = getattr(args, \"masked_dist_loss\", -1.0) args.x_norm_loss = getattr(args, \"x_norm_loss\", -1.0) args.delta_pair_repr_norm_loss = getattr(args, \"delta_pair_repr_norm_loss\", -1.0) @register_model_architecture(\"unimol\", \"unimol_base\") def unimol_base_architecture(args): base_architecture(args)","title":"unimol.py"},{"location":"document/#transformer_encoder_with_pairpy","text":"# Copyright (c) DP Technology. # This source code is licensed under the MIT license found in the # LICENSE file in the root directory of this source tree. from typing import Optional import math import torch import torch.nn as nn import torch.nn.functional as F from unicore.modules import TransformerEncoderLayer, LayerNorm class TransformerEncoderWithPair(nn.Module): def __init__( self, encoder_layers: int = 6, embed_dim: int = 768, ffn_embed_dim: int = 3072, attention_heads: int = 8, emb_dropout: float = 0.1, dropout: float = 0.1, attention_dropout: float = 0.1, activation_dropout: float = 0.0, max_seq_len: int = 256, activation_fn: str = \"gelu\", post_ln: bool = False, no_final_head_layer_norm: bool = False, ) -> None: super().__init__() self.emb_dropout = emb_dropout self.max_seq_len = max_seq_len self.embed_dim = embed_dim self.attention_heads = attention_heads self.emb_layer_norm = LayerNorm(self.embed_dim) if not post_ln: self.final_layer_norm = LayerNorm(self.embed_dim) else: self.final_layer_norm = None if not no_final_head_layer_norm: self.final_head_layer_norm = LayerNorm(attention_heads) else: self.final_head_layer_norm = None self.layers = nn.ModuleList( [ TransformerEncoderLayer( embed_dim=self.embed_dim, ffn_embed_dim=ffn_embed_dim, attention_heads=attention_heads, dropout=dropout, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn, post_ln=post_ln, ) for _ in range(encoder_layers) ] ) def forward( self, emb: torch.Tensor, attn_mask: Optional[torch.Tensor] = None, padding_mask: Optional[torch.Tensor] = None, ) -> torch.Tensor: bsz = emb.size(0) seq_len = emb.size(1) x = self.emb_layer_norm(emb) x = F.dropout(x, p=self.emb_dropout, training=self.training) # account for padding while computing the representation if padding_mask is not None: # \u9884\u8bad\u7ec3\u4e2d\uff0c\u83b7\u53d6\u63a9\u76d6\u4e4b\u540e\u7684atom representation x = x * (1 - padding_mask.unsqueeze(-1).type_as(x)) input_attn_mask = attn_mask input_padding_mask = padding_mask def fill_attn_mask(attn_mask, padding_mask, fill_val=float(\"-inf\")): if attn_mask is not None and padding_mask is not None: # merge key_padding_mask and attn_mask attn_mask = attn_mask.view(x.size(0), -1, seq_len, seq_len) attn_mask.masked_fill_( padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), fill_val, ) attn_mask = attn_mask.view(-1, seq_len, seq_len) padding_mask = None return attn_mask, padding_mask assert attn_mask is not None # \u83b7\u53d6\u63a9\u76d6\u4e4b\u540e\u7684pair representation attn_mask, padding_mask = fill_attn_mask(attn_mask, padding_mask) # \u4fe1\u606f\u5728\u5c42\u95f4\u4f20\u64ad\u8fc7\u7a0b\u4e2d\uff0c\u7ef4\u6301atom representation\u548cpair representation\u7684\u5b58\u5728 for i in range(len(self.layers)): x, attn_mask, _ = self.layers[i]( x, padding_mask=padding_mask, attn_bias=attn_mask, return_attn=True ) def norm_loss(x, eps=1e-10, tolerance=1.0): x = x.float() max_norm = x.shape[-1] ** 0.5 norm = torch.sqrt(torch.sum(x**2, dim=-1) + eps) error = torch.nn.functional.relu((norm - max_norm).abs() - tolerance) return error def masked_mean(mask, value, dim=-1, eps=1e-10): return ( torch.sum(mask * value, dim=dim) / (eps + torch.sum(mask, dim=dim)) ).mean() x_norm = norm_loss(x) if input_padding_mask is not None: token_mask = 1.0 - input_padding_mask.float() else: token_mask = torch.ones_like(x_norm, device=x_norm.device) x_norm = masked_mean(token_mask, x_norm) if self.final_layer_norm is not None: x = self.final_layer_norm(x) delta_pair_repr = attn_mask - input_attn_mask delta_pair_repr, _ = fill_attn_mask(delta_pair_repr, input_padding_mask, 0) attn_mask = ( attn_mask.view(bsz, -1, seq_len, seq_len).permute(0, 2, 3, 1).contiguous() ) delta_pair_repr = ( delta_pair_repr.view(bsz, -1, seq_len, seq_len) .permute(0, 2, 3, 1) .contiguous() ) pair_mask = token_mask[..., None] * token_mask[..., None, :] delta_pair_repr_norm = norm_loss(delta_pair_repr) delta_pair_repr_norm = masked_mean( pair_mask, delta_pair_repr_norm, dim=(-1, -2) ) if self.final_head_layer_norm is not None: delta_pair_repr = self.final_head_layer_norm(delta_pair_repr) return x, attn_mask, delta_pair_repr, x_norm, delta_pair_repr_norm","title":"transformer_encoder_with_pair.py"},{"location":"document/#transformer_encoder_layerpy","text":"from typing import Dict, Optional import torch import torch.nn.functional as F from unicore import utils from torch import nn from . import LayerNorm, SelfMultiheadAttention class TransformerEncoderLayer(nn.Module): \"\"\" Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained models. \"\"\" def __init__( self, embed_dim: int = 768, ffn_embed_dim: int = 3072, attention_heads: int = 8, dropout: float = 0.1, attention_dropout: float = 0.1, activation_dropout: float = 0.0, activation_fn: str = \"gelu\", post_ln = False, ) -> None: super().__init__() # Initialize parameters self.embed_dim = embed_dim self.attention_heads = attention_heads self.attention_dropout = attention_dropout self.dropout = dropout self.activation_dropout = activation_dropout self.activation_fn = utils.get_activation_fn(activation_fn) self.self_attn = SelfMultiheadAttention( self.embed_dim, attention_heads, dropout=attention_dropout, ) # layer norm associated with the self attention layer self.self_attn_layer_norm = LayerNorm(self.embed_dim) self.fc1 = nn.Linear(self.embed_dim, ffn_embed_dim) self.fc2 = nn.Linear(ffn_embed_dim, self.embed_dim) self.final_layer_norm = LayerNorm(self.embed_dim) self.post_ln = post_ln def forward( self, x: torch.Tensor, attn_bias: Optional[torch.Tensor] = None, padding_mask: Optional[torch.Tensor] = None, return_attn: bool=False, ) -> torch.Tensor: \"\"\" LayerNorm is applied either before or after the self-attention/ffn modules similar to the original Transformer implementation. \"\"\" residual = x # \u5c42\u6b63\u5219\u5316\u7684\u4f5c\u7528 if not self.post_ln: x = self.self_attn_layer_norm(x) x = self.self_attn( query=x, key_padding_mask=padding_mask, attn_bias=attn_bias, return_attn=return_attn, ) if return_attn: # \u5982\u679creturn_attn==True\uff0c\u6b64\u65f6\u7684x = [o, attn_weights, attn] x, attn_weights, attn_probs = x x = F.dropout(x, p=self.dropout, training=self.training) x = residual + x if self.post_ln: x = self.self_attn_layer_norm(x) residual = x if not self.post_ln: x = self.final_layer_norm(x) x = self.fc1(x) x = self.activation_fn(x) x = F.dropout(x, p=self.activation_dropout, training=self.training) x = self.fc2(x) x = F.dropout(x, p=self.dropout, training=self.training) x = residual + x if self.post_ln: x = self.final_layer_norm(x) if not return_attn: return x else: return x, attn_weights, attn_probs","title":"transformer_encoder_layer.py"},{"location":"document/#multihead_attentionpy","text":"class SelfMultiheadAttention(nn.Module): def __init__( self, embed_dim, num_heads, dropout=0.1, bias=True, scaling_factor=1, ): super().__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.dropout = dropout self.head_dim = embed_dim // num_heads assert ( self.head_dim * num_heads == self.embed_dim ), \"embed_dim must be divisible by num_heads\" self.scaling = (self.head_dim * scaling_factor) ** -0.5 self.in_proj = nn.Linear(embed_dim, embed_dim * 3, bias=bias) self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias) def forward( self, query, key_padding_mask: Optional[Tensor] = None, attn_bias: Optional[Tensor] = None, return_attn: bool = False, ) -> Tensor: bsz, tgt_len, embed_dim = query.size() assert embed_dim == self.embed_dim q, k, v = self.in_proj(query).chunk(3, dim=-1) q = ( q.view(bsz, tgt_len, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) * self.scaling ) if k is not None: k = ( k.view(bsz, -1, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) ) if v is not None: v = ( v.view(bsz, -1, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) ) assert k is not None src_len = k.size(1) # This is part of a workaround to get around fork/join parallelism # not supporting Optional types. if key_padding_mask is not None and key_padding_mask.dim() == 0: key_padding_mask = None if key_padding_mask is not None: assert key_padding_mask.size(0) == bsz assert key_padding_mask.size(1) == src_len attn_weights = torch.bmm(q, k.transpose(1, 2)) assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len] if key_padding_mask is not None: # don't attend to padding symbols attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) attn_weights.masked_fill_( key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float(\"-inf\") ) attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len) if not return_attn: attn = softmax_dropout( attn_weights, self.dropout, self.training, bias=attn_bias, ) else: # \u516c\u5f0f2\u5bf9\u5e94\u7684\u4ee3\u7801 attn_weights += attn_bias attn = softmax_dropout( attn_weights, self.dropout, self.training, inplace=False, ) o = torch.bmm(attn, v) assert list(o.size()) == [bsz * self.num_heads, tgt_len, self.head_dim] o = ( o.view(bsz, self.num_heads, tgt_len, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz, tgt_len, embed_dim) ) o = self.out_proj(o) if not return_attn: return o else: # attn_weights\u4ee3\u8868\u539f\u5b50\u95f4\u7684attention\u77e9\u9635 # \u8f93\u51fa\u540e\u4f5c\u4e3a\u4e0b\u4e00\u5c42\u7684pair repretention # \u5bf9\u5e94\u4e8e\u516c\u5f0f1 return o, attn_weights, attn","title":"multihead_attention.py"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/","text":"\u9879\u76ee\u6d41\u7a0b\u4e0e\u7ed3\u679c \u51c6\u5907\u5de5\u4f5c \u65b0\u5efaAnaconda\u865a\u62df\u73af\u5883 conda create -n unimol python=3.8 conda activate unimol pip3 install torch torchvision torchaudio pip3 install rdkit pip3 install deepchem \u4e0b\u8f7dUni-Mol # \u5b89\u88c5Uni-Core wget https://github.com/dptech-corp/Uni-Core/archive/refs/heads/main.zip unzip main.zip cd Uni-Core-main python setup.py install # \u5b89\u88c5Uni-Mol wget https://github.com/dptech-corp/Uni-Mol/archive/refs/heads/main.zip unzip main.zip cd Uni-Mol-main/unimol python setup.py install #\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd wget https://github.com/dptech-corp/Uni-Mol/releases/download/v0.1/mol_pre_no_h_220816.pt \u4e0b\u8f7dBBBP.csv\u6587\u4ef6 #\u4eceMoleculeNet\u4e0b\u8f7dBBBP.csv mkdir raw_data cd raw_data wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv \u6784\u5efa\u6570\u636e\u96c6 import deepchem as dc import pandas as pd import numpy from rdkit import Chem df = pd.read_csv(\"./raw_data/BBBP.csv\") # \u53bb\u9664\u4e0d\u80fd\u88abrdkit\u8bfb\u53d6\u7684\u5206\u5b50 ys = [] smiles = [] for label, smi in zip(df['p_np'].to_list(),df['smiles'].to_list()): mol = Chem.MolFromSmiles(smi) if mol is not None: ys.append(label) smiles.append(smi) # \u6784\u5efa\u65b0\u7684\u6570\u636e\u96c6 Xs = numpy.ones_like(ys) dataset = dc.data.NumpyDataset(X=Xs,y=ys,ids=smiles) # \u5212\u5206\u6570\u636e\u96c6 scaffoldsplitter = dc.splits.ScaffoldSplitter() train, valid, test = scaffoldsplitter.train_valid_test_split(dataset, 0.8, 0.1, 0.1) def dataset2df(dataset): data = {\"smiles\": list(dataset.ids), \"p_np\":list(dataset.y), } df = pd.DataFrame(data) return df # \u4fee\u6539\u4e86write_lmdb\u51fd\u6570\uff0c\u5c06\u4f20\u5165\u53d8\u91cf\u7531CSV\u6587\u4ef6\u53d8\u4e3a\u5df2\u7ecf\u7531deepChem\u5212\u5206\u597d\u7684\u6570\u636e\u96c6 def write_lmdb(train, valid, test, outpath='./', nthreads=16): train, valid, test = [dataset2df(dataset) for dataset in [train, valid, test]] for name, content_list in [('train.lmdb', zip(*[train[c].values.tolist() for c in train])), ('valid.lmdb', zip(*[valid[c].values.tolist() for c in valid])), ('test.lmdb', zip(*[test[c].values.tolist() for c in test]))]: os.makedirs(outpath, exist_ok=True) output_name = os.path.join(outpath, name) try: os.remove(output_name) except: pass env_new = lmdb.open( output_name, subdir=False, readonly=False, lock=False, readahead=False, meminit=False, max_readers=1, map_size=int(100e9), ) txn_write = env_new.begin(write=True) with Pool(nthreads) as pool: i = 0 for inner_output in tqdm(pool.imap(smi2coords, content_list)): if inner_output is not None: txn_write.put(f'{i}'.encode(\"ascii\"), inner_output) i += 1 print('{} process {} lines'.format(name, i)) txn_write.commit() env_new.close() write_lmdb(train, valid, test, outpath='./BBBP', nthreads=8) \u7ed3\u5408\u8d85\u53c2\u6570\u641c\u7d22\u7684finetune \u4e3a\u4e86\u4f7f\u5f97\u6a21\u578b\u83b7\u5f97\u6700\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5728BBBP\u8bad\u7ec3\u96c6\u4e0a\u8fdb\u884c\u53c2\u6570\u4f18\u5316\u7684\u540c\u65f6\uff0c\u9488\u5bf9\u5b66\u4e60\u7387lr\u3001\u6279\u6837\u672c\u6570\u3001\u8bad\u7ec3\u8f6e\u6570\u3001dropout\u3001wramup\u8fd95\u4e2a\u8d85\u53c2\u6570\u8fdb\u884c\u7f51\u683c\u641c\u7d22\u3002 data_path='./' save_dir='./save_demo' MASTER_PORT=10086 n_gpu=1 dict_name='dict.txt' weight_path='./weights/mol_pre_no_h_220816.pt' task_name='BBBP' # data folder name task_num=2 loss_func='finetune_cross_entropy' local_batch_size=32 only_polar=0 # -1 all h; 0 no h conf_size=11 seed=0 metric=\"valid_agg_auc\" cp example_data/molecule/$dict_name $data_path export NCCL_ASYNC_ERROR_HANDLING=1 export OMP_NUM_THREADS=1 for lr in 1e-4 4e-4 5e-4; do for batch_size in 64 128 256; do for epoch in 40 60; do for dropout in 0.0 0.1; do for warmup in 0.0 0.06 0.1; do update_freq=`expr $batch_size / $local_batch_size` save_dir='./save_demo/'$lr'_'$batch_size'_'$epoch'_'$dropout'_'$warmup python -m torch.distributed.launch --nproc_per_node=$n_gpu --master_port=$MASTER_PORT --use_env \\ $(which unicore-train) $data_path --task-name $task_name \\ --user-dir ../unimol --train-subset train --valid-subset valid \\ --conf-size $conf_size \\ --num-workers 8 --ddp-backend=c10d \\ --dict-name $dict_name \\ --task mol_finetune --loss $loss_func --arch unimol_base \\ --classification-head-name $task_name --num-classes $task_num \\ --optimizer adam --adam-betas '(0.9, 0.99)' --adam-eps 1e-6 --clip-norm 1.0 \\ --lr-scheduler polynomial_decay --lr $lr --warmup-ratio $warmup --max-epoch $epoch --batch-size $local_batch_size --pooler-dropout $dropout\\ --update-freq $update_freq --seed $seed \\ --fp16 --fp16-init-scale 4 --fp16-scale-window 256 \\ --log-interval 100 --log-format simple \\ --validate-interval 1 --keep-last-epochs 10 \\ --finetune-from-model $weight_path \\ --best-checkpoint-metric $metric --patience 20 \\ --save-dir $save_dir --only-polar $only_polar \\ --maximize-best-checkpoint-metric done done done done done \u57fa\u4e8eAUC\u7684\u6a21\u578b\u8bc4\u4f30 \u6839\u636eAUC\u8fdb\u884c\u8bc4\u4f30\uff0c\u5f53\u8d85\u53c2\u6570lr=5e-4\uff0cbatch_size=256\uff0cepoch=40\uff0cdropout=0.1\uff0cwarmup=0.0\u65f6\uff0c\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u6700\u597d\uff0cAUC=0.746\uff0c\u8fd9\u91cc\u662f\u5bf9\u5e94\u7684 \u9884\u6d4b\u7ed3\u679c(csv\u683c\u5f0f) \u3002 \u4ee5\u4e0b\u662f\u7528\u4e8e\u5c06\u9884\u6d4b\u7ed3\u679c\u8bfb\u53d6\u4e3acsv\u683c\u5f0f\u5e76\u8ba1\u7b97AUC\u7684\u4ee3\u7801\uff1a import pandas as pd import numpy as np from sklearn.metrics import roc_auc_score import sys dirs = sys.argv[1] def get_csv_results(predict_path, csv_path): predict = pd.read_pickle(predict_path) smi_list, predict_list = [], [] for batch in predict: sz = batch[\"bsz\"] for i in range(sz): smi_list.append(batch[\"smi_name\"][i]) predict_list.append(batch[\"prob\"][i][1].cpu().tolist()) predict_df = pd.DataFrame({\"smiles\": smi_list, \"predict_prob\": predict_list}) predict_df = predict_df.groupby(\"smiles\")[\"predict_prob\"].mean().reset_index() predict_df.to_csv(csv_path,index=False) return predict_df predict_path='./infer_demo/%s_test.out.pkl'%dirs csv_path='./infer_demo/%s.csv'%dirs predict_df = get_csv_results(predict_path, csv_path) data_df = pd.read_csv('./BBBP/test.csv') total_df = pd.merge(data_df, predict_df, on='smiles') y = total_df['p_np'].to_list() y_pred = total_df['predict_prob'].to_list() score = roc_auc_score(y, y_pred) print(\"AUC = %s\"%score)","title":"\u9879\u76ee\u6d41\u7a0b\u4e0e\u7ed3\u679c"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/#_1","text":"","title":"\u9879\u76ee\u6d41\u7a0b\u4e0e\u7ed3\u679c"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/#_2","text":"\u65b0\u5efaAnaconda\u865a\u62df\u73af\u5883 conda create -n unimol python=3.8 conda activate unimol pip3 install torch torchvision torchaudio pip3 install rdkit pip3 install deepchem \u4e0b\u8f7dUni-Mol # \u5b89\u88c5Uni-Core wget https://github.com/dptech-corp/Uni-Core/archive/refs/heads/main.zip unzip main.zip cd Uni-Core-main python setup.py install # \u5b89\u88c5Uni-Mol wget https://github.com/dptech-corp/Uni-Mol/archive/refs/heads/main.zip unzip main.zip cd Uni-Mol-main/unimol python setup.py install #\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd wget https://github.com/dptech-corp/Uni-Mol/releases/download/v0.1/mol_pre_no_h_220816.pt \u4e0b\u8f7dBBBP.csv\u6587\u4ef6 #\u4eceMoleculeNet\u4e0b\u8f7dBBBP.csv mkdir raw_data cd raw_data wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv \u6784\u5efa\u6570\u636e\u96c6 import deepchem as dc import pandas as pd import numpy from rdkit import Chem df = pd.read_csv(\"./raw_data/BBBP.csv\") # \u53bb\u9664\u4e0d\u80fd\u88abrdkit\u8bfb\u53d6\u7684\u5206\u5b50 ys = [] smiles = [] for label, smi in zip(df['p_np'].to_list(),df['smiles'].to_list()): mol = Chem.MolFromSmiles(smi) if mol is not None: ys.append(label) smiles.append(smi) # \u6784\u5efa\u65b0\u7684\u6570\u636e\u96c6 Xs = numpy.ones_like(ys) dataset = dc.data.NumpyDataset(X=Xs,y=ys,ids=smiles) # \u5212\u5206\u6570\u636e\u96c6 scaffoldsplitter = dc.splits.ScaffoldSplitter() train, valid, test = scaffoldsplitter.train_valid_test_split(dataset, 0.8, 0.1, 0.1) def dataset2df(dataset): data = {\"smiles\": list(dataset.ids), \"p_np\":list(dataset.y), } df = pd.DataFrame(data) return df # \u4fee\u6539\u4e86write_lmdb\u51fd\u6570\uff0c\u5c06\u4f20\u5165\u53d8\u91cf\u7531CSV\u6587\u4ef6\u53d8\u4e3a\u5df2\u7ecf\u7531deepChem\u5212\u5206\u597d\u7684\u6570\u636e\u96c6 def write_lmdb(train, valid, test, outpath='./', nthreads=16): train, valid, test = [dataset2df(dataset) for dataset in [train, valid, test]] for name, content_list in [('train.lmdb', zip(*[train[c].values.tolist() for c in train])), ('valid.lmdb', zip(*[valid[c].values.tolist() for c in valid])), ('test.lmdb', zip(*[test[c].values.tolist() for c in test]))]: os.makedirs(outpath, exist_ok=True) output_name = os.path.join(outpath, name) try: os.remove(output_name) except: pass env_new = lmdb.open( output_name, subdir=False, readonly=False, lock=False, readahead=False, meminit=False, max_readers=1, map_size=int(100e9), ) txn_write = env_new.begin(write=True) with Pool(nthreads) as pool: i = 0 for inner_output in tqdm(pool.imap(smi2coords, content_list)): if inner_output is not None: txn_write.put(f'{i}'.encode(\"ascii\"), inner_output) i += 1 print('{} process {} lines'.format(name, i)) txn_write.commit() env_new.close() write_lmdb(train, valid, test, outpath='./BBBP', nthreads=8)","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/#finetune","text":"\u4e3a\u4e86\u4f7f\u5f97\u6a21\u578b\u83b7\u5f97\u6700\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5728BBBP\u8bad\u7ec3\u96c6\u4e0a\u8fdb\u884c\u53c2\u6570\u4f18\u5316\u7684\u540c\u65f6\uff0c\u9488\u5bf9\u5b66\u4e60\u7387lr\u3001\u6279\u6837\u672c\u6570\u3001\u8bad\u7ec3\u8f6e\u6570\u3001dropout\u3001wramup\u8fd95\u4e2a\u8d85\u53c2\u6570\u8fdb\u884c\u7f51\u683c\u641c\u7d22\u3002 data_path='./' save_dir='./save_demo' MASTER_PORT=10086 n_gpu=1 dict_name='dict.txt' weight_path='./weights/mol_pre_no_h_220816.pt' task_name='BBBP' # data folder name task_num=2 loss_func='finetune_cross_entropy' local_batch_size=32 only_polar=0 # -1 all h; 0 no h conf_size=11 seed=0 metric=\"valid_agg_auc\" cp example_data/molecule/$dict_name $data_path export NCCL_ASYNC_ERROR_HANDLING=1 export OMP_NUM_THREADS=1 for lr in 1e-4 4e-4 5e-4; do for batch_size in 64 128 256; do for epoch in 40 60; do for dropout in 0.0 0.1; do for warmup in 0.0 0.06 0.1; do update_freq=`expr $batch_size / $local_batch_size` save_dir='./save_demo/'$lr'_'$batch_size'_'$epoch'_'$dropout'_'$warmup python -m torch.distributed.launch --nproc_per_node=$n_gpu --master_port=$MASTER_PORT --use_env \\ $(which unicore-train) $data_path --task-name $task_name \\ --user-dir ../unimol --train-subset train --valid-subset valid \\ --conf-size $conf_size \\ --num-workers 8 --ddp-backend=c10d \\ --dict-name $dict_name \\ --task mol_finetune --loss $loss_func --arch unimol_base \\ --classification-head-name $task_name --num-classes $task_num \\ --optimizer adam --adam-betas '(0.9, 0.99)' --adam-eps 1e-6 --clip-norm 1.0 \\ --lr-scheduler polynomial_decay --lr $lr --warmup-ratio $warmup --max-epoch $epoch --batch-size $local_batch_size --pooler-dropout $dropout\\ --update-freq $update_freq --seed $seed \\ --fp16 --fp16-init-scale 4 --fp16-scale-window 256 \\ --log-interval 100 --log-format simple \\ --validate-interval 1 --keep-last-epochs 10 \\ --finetune-from-model $weight_path \\ --best-checkpoint-metric $metric --patience 20 \\ --save-dir $save_dir --only-polar $only_polar \\ --maximize-best-checkpoint-metric done done done done done","title":"\u7ed3\u5408\u8d85\u53c2\u6570\u641c\u7d22\u7684finetune"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/#auc","text":"\u6839\u636eAUC\u8fdb\u884c\u8bc4\u4f30\uff0c\u5f53\u8d85\u53c2\u6570lr=5e-4\uff0cbatch_size=256\uff0cepoch=40\uff0cdropout=0.1\uff0cwarmup=0.0\u65f6\uff0c\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u6700\u597d\uff0cAUC=0.746\uff0c\u8fd9\u91cc\u662f\u5bf9\u5e94\u7684 \u9884\u6d4b\u7ed3\u679c(csv\u683c\u5f0f) \u3002 \u4ee5\u4e0b\u662f\u7528\u4e8e\u5c06\u9884\u6d4b\u7ed3\u679c\u8bfb\u53d6\u4e3acsv\u683c\u5f0f\u5e76\u8ba1\u7b97AUC\u7684\u4ee3\u7801\uff1a import pandas as pd import numpy as np from sklearn.metrics import roc_auc_score import sys dirs = sys.argv[1] def get_csv_results(predict_path, csv_path): predict = pd.read_pickle(predict_path) smi_list, predict_list = [], [] for batch in predict: sz = batch[\"bsz\"] for i in range(sz): smi_list.append(batch[\"smi_name\"][i]) predict_list.append(batch[\"prob\"][i][1].cpu().tolist()) predict_df = pd.DataFrame({\"smiles\": smi_list, \"predict_prob\": predict_list}) predict_df = predict_df.groupby(\"smiles\")[\"predict_prob\"].mean().reset_index() predict_df.to_csv(csv_path,index=False) return predict_df predict_path='./infer_demo/%s_test.out.pkl'%dirs csv_path='./infer_demo/%s.csv'%dirs predict_df = get_csv_results(predict_path, csv_path) data_df = pd.read_csv('./BBBP/test.csv') total_df = pd.merge(data_df, predict_df, on='smiles') y = total_df['p_np'].to_list() y_pred = total_df['predict_prob'].to_list() score = roc_auc_score(y, y_pred) print(\"AUC = %s\"%score)","title":"\u57fa\u4e8eAUC\u7684\u6a21\u578b\u8bc4\u4f30"}]}