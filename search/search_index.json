{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to UM-BBB UM-BBB\u662f\u672c\u4eba\u5728Uni-Mol\u57fa\u7840\u4e0a\u7528\u4e8e\u9879\u76ee\u7ec3\u4e60\u7684\u5b9e\u8df5\u4ea7\u7269\uff0c\u5173\u4e8eUni-Mol\u7684\u8be6\u7ec6\u4fe1\u606f\u8bf7\u8bbf\u95ee uni-mol\u5b98\u65b9\u7f51\u5740 . \u4f5c\u8005\u4fe1\u606f \u59d3\u540d \u2003\u6768\u5408 \u6559\u80b2\u80cc\u666f \u2003\u672c\u79d1 \u4e2d\u5c71\u5927\u5b66 \u836f\u5b66\u9662 \u836f\u5b66\u4e13\u4e1a \u2003\u7855\u58eb \u56fd\u79d1\u5927 \u6df1\u5733\u5148\u8fdb\u6280\u672f\u7814\u7a76\u9662 \u8ba1\u7b97\u751f\u7269\u5b66\u65b9\u5411 \u9879\u76ee\u80cc\u666f\u4e0e\u76ee\u6807 Uni-Mol\u7b80\u4ecb \u3000\u3000\u4ece\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u6570\u636e\u4e2d\u63d0\u53d6\u8868\u5f81\u4fe1\u606f\uff0c\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6b63\u5728\u5e2d\u5377AI\u9886\u57df\uff0c\u5e76\u6210\u4e3a\u5f88\u591a\u9886\u57df\u7684\u4e8b\u5b9e\u89e3\u51b3\u65b9\u6848\uff0c\u5982NLP\u9886\u57df\u7684GPT-3\uff0cCV\u9886\u57df\u7684ViT\u3002 \u3000\u3000Uni-Mol\u662f\u6df1\u52bf\u79d1\u6280\u56e2\u961f\u53d1\u5e03\u76843D\u5206\u5b50\u9884\u8bad\u7ec3\u6a21\u578b\uff0cUni-Mol\u5c06\u5206\u5b503D\u7ed3\u6784\u4f5c\u4e3a\u6a21\u578b\u8f93\u5165\uff0c\u800c\u975e1D\u6216\u80052D\u4fe1\u606f\u3002\u4ece3D\u4fe1\u606f\u51fa\u53d1\u7684\u8868\u5f81\u5b66\u4e60\u8ba9Uni-Mol\u5728\u5173\u4e8e\u836f\u7269\u5206\u5b50\u3001\u86cb\u767d\u7ed3\u5408\u53e3\u888b\u7684\u4f17\u591a\u4e0b\u6e38\u4efb\u52a1\u8fbe\u5230\u4e86SOTA\u3002 \u8840\u8111\u5c4f\u969c\u900f\u8fc7\u80fd\u529b \u3000\u3000\u836f\u7269\u5206\u5b50\u80fd\u5426\u900f\u8fc7\u8840\u8111\u5c4f\u969c(blood brain barrior, BBB)\u662f\u836f\u7269\u7684\u91cd\u8981\u5c5e\u6027\uff0c\u5bf9\u4e8e\u4f5c\u7528\u4e8e\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u836f\u7269\uff0c\u836f\u7269\u5206\u5b50\u5fc5\u987b\u8981\u901a\u8fc7\u8840\u8111\u5c4f\u969c\u624d\u80fd\u5230\u8fbe\u4f5c\u7528\u9776\u70b9\uff0c\u800c\u5bf9\u4e8e\u90a3\u4e9b\u4f5c\u7528\u9776\u70b9\u4e0d\u5728\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u836f\u7269\uff0c\u8fd9\u4e9b\u836f\u7269\u5206\u5b50\u7684\u8840\u8111\u5c4f\u969c\u900f\u8fc7\u80fd\u529b\u8981\u5c3d\u53ef\u80fd\u7684\u4f4e\uff0c\u4ee5\u514d\u4ea7\u751f\u795e\u7ecf\u6bd2\u526f\u4f5c\u7528\u3002 \u9879\u76ee\u76ee\u6807 \u3000\u3000\u5229\u7528\u6df1\u52bf\u79d1\u6280\u56e2\u961f\u63d0\u4f9b\u7684Uni-Mol\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u5bf9\u5e94\u7684BBB\u6570\u636e\u96c6\u4e0a\u8fdb\u884cfinetune\uff0c\u8bc4\u6d4b\u6307\u6807\u662fAUC\u3002","title":"Welcome to UM-BBB"},{"location":"#welcome-to-um-bbb","text":"UM-BBB\u662f\u672c\u4eba\u5728Uni-Mol\u57fa\u7840\u4e0a\u7528\u4e8e\u9879\u76ee\u7ec3\u4e60\u7684\u5b9e\u8df5\u4ea7\u7269\uff0c\u5173\u4e8eUni-Mol\u7684\u8be6\u7ec6\u4fe1\u606f\u8bf7\u8bbf\u95ee uni-mol\u5b98\u65b9\u7f51\u5740 .","title":"Welcome to UM-BBB"},{"location":"#_1","text":"\u59d3\u540d \u2003\u6768\u5408 \u6559\u80b2\u80cc\u666f \u2003\u672c\u79d1 \u4e2d\u5c71\u5927\u5b66 \u836f\u5b66\u9662 \u836f\u5b66\u4e13\u4e1a \u2003\u7855\u58eb \u56fd\u79d1\u5927 \u6df1\u5733\u5148\u8fdb\u6280\u672f\u7814\u7a76\u9662 \u8ba1\u7b97\u751f\u7269\u5b66\u65b9\u5411","title":"\u4f5c\u8005\u4fe1\u606f"},{"location":"#_2","text":"Uni-Mol\u7b80\u4ecb \u3000\u3000\u4ece\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u6570\u636e\u4e2d\u63d0\u53d6\u8868\u5f81\u4fe1\u606f\uff0c\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6b63\u5728\u5e2d\u5377AI\u9886\u57df\uff0c\u5e76\u6210\u4e3a\u5f88\u591a\u9886\u57df\u7684\u4e8b\u5b9e\u89e3\u51b3\u65b9\u6848\uff0c\u5982NLP\u9886\u57df\u7684GPT-3\uff0cCV\u9886\u57df\u7684ViT\u3002 \u3000\u3000Uni-Mol\u662f\u6df1\u52bf\u79d1\u6280\u56e2\u961f\u53d1\u5e03\u76843D\u5206\u5b50\u9884\u8bad\u7ec3\u6a21\u578b\uff0cUni-Mol\u5c06\u5206\u5b503D\u7ed3\u6784\u4f5c\u4e3a\u6a21\u578b\u8f93\u5165\uff0c\u800c\u975e1D\u6216\u80052D\u4fe1\u606f\u3002\u4ece3D\u4fe1\u606f\u51fa\u53d1\u7684\u8868\u5f81\u5b66\u4e60\u8ba9Uni-Mol\u5728\u5173\u4e8e\u836f\u7269\u5206\u5b50\u3001\u86cb\u767d\u7ed3\u5408\u53e3\u888b\u7684\u4f17\u591a\u4e0b\u6e38\u4efb\u52a1\u8fbe\u5230\u4e86SOTA\u3002 \u8840\u8111\u5c4f\u969c\u900f\u8fc7\u80fd\u529b \u3000\u3000\u836f\u7269\u5206\u5b50\u80fd\u5426\u900f\u8fc7\u8840\u8111\u5c4f\u969c(blood brain barrior, BBB)\u662f\u836f\u7269\u7684\u91cd\u8981\u5c5e\u6027\uff0c\u5bf9\u4e8e\u4f5c\u7528\u4e8e\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u836f\u7269\uff0c\u836f\u7269\u5206\u5b50\u5fc5\u987b\u8981\u901a\u8fc7\u8840\u8111\u5c4f\u969c\u624d\u80fd\u5230\u8fbe\u4f5c\u7528\u9776\u70b9\uff0c\u800c\u5bf9\u4e8e\u90a3\u4e9b\u4f5c\u7528\u9776\u70b9\u4e0d\u5728\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u836f\u7269\uff0c\u8fd9\u4e9b\u836f\u7269\u5206\u5b50\u7684\u8840\u8111\u5c4f\u969c\u900f\u8fc7\u80fd\u529b\u8981\u5c3d\u53ef\u80fd\u7684\u4f4e\uff0c\u4ee5\u514d\u4ea7\u751f\u795e\u7ecf\u6bd2\u526f\u4f5c\u7528\u3002 \u9879\u76ee\u76ee\u6807 \u3000\u3000\u5229\u7528\u6df1\u52bf\u79d1\u6280\u56e2\u961f\u63d0\u4f9b\u7684Uni-Mol\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u5bf9\u5e94\u7684BBB\u6570\u636e\u96c6\u4e0a\u8fdb\u884cfinetune\uff0c\u8bc4\u6d4b\u6307\u6807\u662fAUC\u3002","title":"\u9879\u76ee\u80cc\u666f\u4e0e\u76ee\u6807"},{"location":"document/","text":"\u6280\u672f\u6587\u6863 \u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3Uni-Mol\uff0c\u6587\u6863\u7684\u524d\u534a\u90e8\u5206\u6839\u636e\u6587\u7ae0\u5185\u5bb9\uff0c\u4f7f\u7528\u516c\u5f0f\u548c\u603b\u7ed3\u6027\u8bed\u53e5\u63cf\u8ff0\u4e86Uni-Mol\u7684\u8bbe\u8ba1\u601d\u8def\uff0c\u6587\u6863\u7684\u540e\u534a\u90e8\u5206\u5219\u5c55\u793a\u4e86\u4e00\u4e9b\u5173\u952e\u516c\u5f0f\u548c\u6a21\u578b\u601d\u8def\u6240\u5bf9\u5e94\u7684\u4ee3\u7801 Uni-Mol\u6587\u7ae0\u5173\u4e8e\u6a21\u578b\u7684\u9610\u8ff0 \u4e0a\u56fe\u662fUni-Mol\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67b6\u6784\u56fe\uff0c\u53ef\u4ee5\u770b\u51faUni-Mol\u548c\u7ecf\u5178Transformer\u6a21\u578b\u7684\u5f02\u540c\u4e4b\u5904\u5728\u4e8e\uff1a \u8f93\u5165 \u7531\u4e8e\u672c\u8eab\u7684\u7f6e\u6362\u4e0d\u53d8\u6027\uff0cTransformer\u6a21\u578b\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u4f4d\u7f6e\u6ca1\u6709\u5206\u8fa8\u80fd\u529b\uff0c\u4e3a\u4e86\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u5316\u5408\u7269\u5206\u5b50\u76843D\u4fe1\u606f\uff0cUni-Mol\u5c06\u5750\u6807\u4fe1\u606f\u5d4c\u5165\u5230\u4fe1\u606f\u4f20\u64ad\u8fc7\u7a0b\uff0c\u6240\u4ee5Uni-Mol\u6709\u4e24\u4e2a\u8f93\u5165\uff0c\u5206\u522b\u662fatom types\u548catom coordinates\u3002 \u5728\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\uff0cUni-Mol\u548cTransformer\u90fd\u9700\u8981\u5bf9\u8f93\u5165\u8fdb\u884cmask\u64cd\u4f5c\uff0cUni-Mol\u5173\u4e8e\u5750\u6807\u7684mask\u4f7f\u7528\u4e86 noise range \u65b9\u6cd5\uff0c\u6240\u8c13 noise range \u662f\u6307\u5728\u539f\u59cb\u5750\u6807\u4e0a\u52a0\u4e00\u4e2a\u5904\u4e8e\u4e00\u5b9a\u8303\u56f4\u5185\u7684\u968f\u673a\u5750\u6807\u3002 \u6ce8\u610f\u529b\u5934 Uni-Mol\u6784\u5efa\u4e86\u4e09\u79cd\u6ce8\u610f\u529b\u5934\uff0c\u5206\u522b\u5173\u6ce8\u6a21\u578b\u4e2d\u7684Atom Type\uff0cCoordinates\u4ee5\u53caPair-dist\u4fe1\u606f \u8868\u5f81(representation) \u4e3a\u4e86\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u5316\u5408\u7269\u7ed3\u6784\u4e2d\u539f\u5b50\u4fe1\u606f\u548c\u539f\u5b50\u95f4\u8ddd\u79bb\u4fe1\u606f\uff0cUni-Mol\u5728\u4fe1\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u7ef4\u6301\u4e86\u4e24\u79cd\u8868\u5f81\uff0c\u5206\u522b\u662fAtom Representation\u548cPair Representation\uff0c\u8fd9\u4e24\u79cd\u8868\u5f81\u5728\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u8fdb\u884c\u4fe1\u606f\u4ea4\u6d41\u3002 \\[q_{ij}^{l+1}=q_{ij}^{l}+\\lbrace{\\frac{Q_{i}^{l,h} \\lbrace{K_{i}^{l,h}}\\rbrace^{T}}{\\sqrt{d}}}\\mid h \\in [1,h]\\rbrace \\tag{1}\\] \\[Attention(Q_{i}^{l,h},K_{i}^{l,h},V_{i}^{l,h})=softmax({\\frac{Q_{i}^{l,h}\\lbrace{K_{i}^{l,h}}\\rbrace^{T}}{\\sqrt{d}}} + q_{ij}^{l-1,h})V_{i}^{l,h} \\tag{2}\\] \u9884\u6d4b3D\u5750\u6807 Uni-Mol\u5728\u6a21\u578b\u4e2d\u5f15\u5165\u5173\u6ce8\u539f\u5b50\u95f4\u8ddd\u79bb\u7684\u6ce8\u610f\u529b\u5934(Pair-dist Head)\uff0c\u901a\u8fc7\u8ba1\u7b97\u88ab\u63a9\u76d6\u539f\u5b50\u5750\u6807\u4e0e\u771f\u5b9e\u503c\u7684\u5dee\u5f02\uff0c\u5e76\u4e0e\u5750\u6807\u8f93\u5165\u503c\u76f8\u52a0\uff0c\u5f97\u5230\u539f\u5b50\u7684\u9884\u6d4b\u5750\u6807 \\[\\hat{x_i}=x_i+\\sum_{j = 0}^{n}\\frac{(x_i-x_j)C_{ij}}{n} \\tag{3}\\] \\[c_{ij}=ReLU((q_{ij}^{L}-q_{ij}^{0})U)W \\tag{4}\\] Uni-Mol\u7684\u4ee3\u8868\u6027\u4ee3\u7801 \u4f7f\u7528\u9ad8\u65af\u6838\u51fd\u6570\u5904\u7406\u539f\u5b50\u95f4\u8ddd\u79bb\u4fe1\u606f def gaussian(x, mean, std): pi = 3.14159 a = (2 * pi) ** 0.5 return torch.exp(-0.5 * (((x - mean) / std) ** 2)) / (a * std) class GaussianLayer(nn.Module): def __init__(self, K=128, edge_types=1024): super().__init__() self.K = K self.means = nn.Embedding(1, K) self.stds = nn.Embedding(1, K) self.mul = nn.Embedding(edge_types, 1) self.bias = nn.Embedding(edge_types, 1) nn.init.uniform_(self.means.weight, 0, 3) nn.init.uniform_(self.stds.weight, 0, 3) nn.init.constant_(self.bias.weight, 0) nn.init.constant_(self.mul.weight, 1) def forward(self, x, edge_type): mul = self.mul(edge_type).type_as(x) bias = self.bias(edge_type).type_as(x) x = mul * x.unsqueeze(-1) + bias x = x.expand(-1, -1, -1, self.K) mean = self.means.weight.float().view(-1) std = self.stds.weight.float().view(-1).abs() + 1e-5 return gaussian(x.float(), mean, std).type_as(self.means.weight) def get_dist_features(dist, et): n_node = dist.size(-1) gbf_feature = self.gbf(dist, et) gbf_result = self.gbf_proj(gbf_feature) graph_attn_bias = gbf_result graph_attn_bias = graph_attn_bias.permute(0, 3, 1, 2).contiguous() graph_attn_bias = graph_attn_bias.view(-1, n_node, n_node) return graph_attn_bias graph_attn_bias = get_dist_features(src_distance, src_edge_type) \u516c\u5f0f1\u6240\u5bf9\u5e94\u7684\u4ee3\u7801 attn_weights = torch.bmm(q, k.transpose(1, 2)) if key_padding_mask is not None: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) attn_weights.masked_fill_( key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float(\"-inf\") ) attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len) if return_attn: attn_weights += attn_bias attn = softmax_dropout( attn_weights, self.dropout, self.training, inplace=False, ) \u516c\u5f0f2\u5bf9\u5e94\u7684\u4ee3\u7801\uff0c\u6765\u81eaUni-Core\u3002 q, k, v = self.in_proj(query).chunk(3, dim=-1) q = ( q.view(bsz, tgt_len, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) * self.scaling ) if k is not None: k = ( k.view(bsz, -1, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) ) if v is not None: v = ( v.view(bsz, -1, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) ) attn_weights = torch.bmm(q, k.transpose(1, 2)) attn_weights += attn_bias attn = softmax_dropout( attn_weights, self.dropout, self.training, inplace=False, ) \u516c\u5f0f3\u6240\u5bf9\u5e94\u7684\u4ee3\u7801\uff0c\u6765\u81eaunimol.py\u3002 self.pair2coord_proj = NonLinearHead( args.encoder_attention_heads, 1, args.activation_fn ) delta_pos = coords_emb.unsqueeze(1) - coords_emb.unsqueeze(2) attn_probs = self.pair2coord_proj(delta_encoder_pair_rep) coord_update = delta_pos / atom_num * attn_probs coord_update = torch.sum(coord_update, dim=2) encoder_coord = coords_emb + coord_update class NonLinearHead(nn.Module): \"\"\"Head for simple classification tasks.\"\"\" def __init__( self, input_dim, out_dim, activation_fn, hidden=None, ): super().__init__() hidden = input_dim if not hidden else hidden self.linear1 = nn.Linear(input_dim, hidden) self.linear2 = nn.Linear(hidden, out_dim) self.activation_fn = utils.get_activation_fn(activation_fn) def forward(self, x): x = self.linear1(x) x = self.activation_fn(x) x = self.linear2(x) return x \u516c\u5f0f4\u5bf9\u5e94\u7684\u4ee3\u7801\uff0c\u6765\u81eatransformer_encoder_with_pair.py input_attn_mask = attn_mask for i in range(len(self.layers)): x, attn_mask, _ = self.layers[i]( x, padding_mask=padding_mask, attn_bias=attn_mask, return_attn=True ) if self.final_layer_norm is not None: x = self.final_layer_norm(x) delta_pair_repr = attn_mask - input_attn_mask","title":"\u6280\u672f\u6587\u6863"},{"location":"document/#_1","text":"\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3Uni-Mol\uff0c\u6587\u6863\u7684\u524d\u534a\u90e8\u5206\u6839\u636e\u6587\u7ae0\u5185\u5bb9\uff0c\u4f7f\u7528\u516c\u5f0f\u548c\u603b\u7ed3\u6027\u8bed\u53e5\u63cf\u8ff0\u4e86Uni-Mol\u7684\u8bbe\u8ba1\u601d\u8def\uff0c\u6587\u6863\u7684\u540e\u534a\u90e8\u5206\u5219\u5c55\u793a\u4e86\u4e00\u4e9b\u5173\u952e\u516c\u5f0f\u548c\u6a21\u578b\u601d\u8def\u6240\u5bf9\u5e94\u7684\u4ee3\u7801","title":"\u6280\u672f\u6587\u6863"},{"location":"document/#uni-mol","text":"\u4e0a\u56fe\u662fUni-Mol\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67b6\u6784\u56fe\uff0c\u53ef\u4ee5\u770b\u51faUni-Mol\u548c\u7ecf\u5178Transformer\u6a21\u578b\u7684\u5f02\u540c\u4e4b\u5904\u5728\u4e8e\uff1a","title":"Uni-Mol\u6587\u7ae0\u5173\u4e8e\u6a21\u578b\u7684\u9610\u8ff0"},{"location":"document/#_2","text":"\u7531\u4e8e\u672c\u8eab\u7684\u7f6e\u6362\u4e0d\u53d8\u6027\uff0cTransformer\u6a21\u578b\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u4f4d\u7f6e\u6ca1\u6709\u5206\u8fa8\u80fd\u529b\uff0c\u4e3a\u4e86\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u5316\u5408\u7269\u5206\u5b50\u76843D\u4fe1\u606f\uff0cUni-Mol\u5c06\u5750\u6807\u4fe1\u606f\u5d4c\u5165\u5230\u4fe1\u606f\u4f20\u64ad\u8fc7\u7a0b\uff0c\u6240\u4ee5Uni-Mol\u6709\u4e24\u4e2a\u8f93\u5165\uff0c\u5206\u522b\u662fatom types\u548catom coordinates\u3002 \u5728\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\uff0cUni-Mol\u548cTransformer\u90fd\u9700\u8981\u5bf9\u8f93\u5165\u8fdb\u884cmask\u64cd\u4f5c\uff0cUni-Mol\u5173\u4e8e\u5750\u6807\u7684mask\u4f7f\u7528\u4e86 noise range \u65b9\u6cd5\uff0c\u6240\u8c13 noise range \u662f\u6307\u5728\u539f\u59cb\u5750\u6807\u4e0a\u52a0\u4e00\u4e2a\u5904\u4e8e\u4e00\u5b9a\u8303\u56f4\u5185\u7684\u968f\u673a\u5750\u6807\u3002","title":"\u8f93\u5165"},{"location":"document/#_3","text":"Uni-Mol\u6784\u5efa\u4e86\u4e09\u79cd\u6ce8\u610f\u529b\u5934\uff0c\u5206\u522b\u5173\u6ce8\u6a21\u578b\u4e2d\u7684Atom Type\uff0cCoordinates\u4ee5\u53caPair-dist\u4fe1\u606f","title":"\u6ce8\u610f\u529b\u5934"},{"location":"document/#representation","text":"\u4e3a\u4e86\u4f7f\u6a21\u578b\u5b66\u4e60\u5230\u5316\u5408\u7269\u7ed3\u6784\u4e2d\u539f\u5b50\u4fe1\u606f\u548c\u539f\u5b50\u95f4\u8ddd\u79bb\u4fe1\u606f\uff0cUni-Mol\u5728\u4fe1\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u7ef4\u6301\u4e86\u4e24\u79cd\u8868\u5f81\uff0c\u5206\u522b\u662fAtom Representation\u548cPair Representation\uff0c\u8fd9\u4e24\u79cd\u8868\u5f81\u5728\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u8fdb\u884c\u4fe1\u606f\u4ea4\u6d41\u3002 \\[q_{ij}^{l+1}=q_{ij}^{l}+\\lbrace{\\frac{Q_{i}^{l,h} \\lbrace{K_{i}^{l,h}}\\rbrace^{T}}{\\sqrt{d}}}\\mid h \\in [1,h]\\rbrace \\tag{1}\\] \\[Attention(Q_{i}^{l,h},K_{i}^{l,h},V_{i}^{l,h})=softmax({\\frac{Q_{i}^{l,h}\\lbrace{K_{i}^{l,h}}\\rbrace^{T}}{\\sqrt{d}}} + q_{ij}^{l-1,h})V_{i}^{l,h} \\tag{2}\\]","title":"\u8868\u5f81(representation)"},{"location":"document/#3d","text":"Uni-Mol\u5728\u6a21\u578b\u4e2d\u5f15\u5165\u5173\u6ce8\u539f\u5b50\u95f4\u8ddd\u79bb\u7684\u6ce8\u610f\u529b\u5934(Pair-dist Head)\uff0c\u901a\u8fc7\u8ba1\u7b97\u88ab\u63a9\u76d6\u539f\u5b50\u5750\u6807\u4e0e\u771f\u5b9e\u503c\u7684\u5dee\u5f02\uff0c\u5e76\u4e0e\u5750\u6807\u8f93\u5165\u503c\u76f8\u52a0\uff0c\u5f97\u5230\u539f\u5b50\u7684\u9884\u6d4b\u5750\u6807 \\[\\hat{x_i}=x_i+\\sum_{j = 0}^{n}\\frac{(x_i-x_j)C_{ij}}{n} \\tag{3}\\] \\[c_{ij}=ReLU((q_{ij}^{L}-q_{ij}^{0})U)W \\tag{4}\\]","title":"\u9884\u6d4b3D\u5750\u6807"},{"location":"document/#uni-mol_1","text":"","title":"Uni-Mol\u7684\u4ee3\u8868\u6027\u4ee3\u7801"},{"location":"document/#_4","text":"def gaussian(x, mean, std): pi = 3.14159 a = (2 * pi) ** 0.5 return torch.exp(-0.5 * (((x - mean) / std) ** 2)) / (a * std) class GaussianLayer(nn.Module): def __init__(self, K=128, edge_types=1024): super().__init__() self.K = K self.means = nn.Embedding(1, K) self.stds = nn.Embedding(1, K) self.mul = nn.Embedding(edge_types, 1) self.bias = nn.Embedding(edge_types, 1) nn.init.uniform_(self.means.weight, 0, 3) nn.init.uniform_(self.stds.weight, 0, 3) nn.init.constant_(self.bias.weight, 0) nn.init.constant_(self.mul.weight, 1) def forward(self, x, edge_type): mul = self.mul(edge_type).type_as(x) bias = self.bias(edge_type).type_as(x) x = mul * x.unsqueeze(-1) + bias x = x.expand(-1, -1, -1, self.K) mean = self.means.weight.float().view(-1) std = self.stds.weight.float().view(-1).abs() + 1e-5 return gaussian(x.float(), mean, std).type_as(self.means.weight) def get_dist_features(dist, et): n_node = dist.size(-1) gbf_feature = self.gbf(dist, et) gbf_result = self.gbf_proj(gbf_feature) graph_attn_bias = gbf_result graph_attn_bias = graph_attn_bias.permute(0, 3, 1, 2).contiguous() graph_attn_bias = graph_attn_bias.view(-1, n_node, n_node) return graph_attn_bias graph_attn_bias = get_dist_features(src_distance, src_edge_type)","title":"\u4f7f\u7528\u9ad8\u65af\u6838\u51fd\u6570\u5904\u7406\u539f\u5b50\u95f4\u8ddd\u79bb\u4fe1\u606f"},{"location":"document/#1","text":"attn_weights = torch.bmm(q, k.transpose(1, 2)) if key_padding_mask is not None: attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) attn_weights.masked_fill_( key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float(\"-inf\") ) attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len) if return_attn: attn_weights += attn_bias attn = softmax_dropout( attn_weights, self.dropout, self.training, inplace=False, )","title":"\u516c\u5f0f1\u6240\u5bf9\u5e94\u7684\u4ee3\u7801"},{"location":"document/#2uni-core","text":"q, k, v = self.in_proj(query).chunk(3, dim=-1) q = ( q.view(bsz, tgt_len, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) * self.scaling ) if k is not None: k = ( k.view(bsz, -1, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) ) if v is not None: v = ( v.view(bsz, -1, self.num_heads, self.head_dim) .transpose(1, 2) .contiguous() .view(bsz * self.num_heads, -1, self.head_dim) ) attn_weights = torch.bmm(q, k.transpose(1, 2)) attn_weights += attn_bias attn = softmax_dropout( attn_weights, self.dropout, self.training, inplace=False, )","title":"\u516c\u5f0f2\u5bf9\u5e94\u7684\u4ee3\u7801\uff0c\u6765\u81eaUni-Core\u3002"},{"location":"document/#3unimolpy","text":"self.pair2coord_proj = NonLinearHead( args.encoder_attention_heads, 1, args.activation_fn ) delta_pos = coords_emb.unsqueeze(1) - coords_emb.unsqueeze(2) attn_probs = self.pair2coord_proj(delta_encoder_pair_rep) coord_update = delta_pos / atom_num * attn_probs coord_update = torch.sum(coord_update, dim=2) encoder_coord = coords_emb + coord_update class NonLinearHead(nn.Module): \"\"\"Head for simple classification tasks.\"\"\" def __init__( self, input_dim, out_dim, activation_fn, hidden=None, ): super().__init__() hidden = input_dim if not hidden else hidden self.linear1 = nn.Linear(input_dim, hidden) self.linear2 = nn.Linear(hidden, out_dim) self.activation_fn = utils.get_activation_fn(activation_fn) def forward(self, x): x = self.linear1(x) x = self.activation_fn(x) x = self.linear2(x) return x","title":"\u516c\u5f0f3\u6240\u5bf9\u5e94\u7684\u4ee3\u7801\uff0c\u6765\u81eaunimol.py\u3002"},{"location":"document/#4transformer_encoder_with_pairpy","text":"input_attn_mask = attn_mask for i in range(len(self.layers)): x, attn_mask, _ = self.layers[i]( x, padding_mask=padding_mask, attn_bias=attn_mask, return_attn=True ) if self.final_layer_norm is not None: x = self.final_layer_norm(x) delta_pair_repr = attn_mask - input_attn_mask","title":"\u516c\u5f0f4\u5bf9\u5e94\u7684\u4ee3\u7801\uff0c\u6765\u81eatransformer_encoder_with_pair.py"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/","text":"\u9879\u76ee\u6d41\u7a0b\u4e0e\u7ed3\u679c \u51c6\u5907\u5de5\u4f5c \u65b0\u5efaAnaconda\u865a\u62df\u73af\u5883 conda create -n unimol python=3.8 conda activate unimol pip3 install torch torchvision torchaudio pip3 install rdkit pip3 install deepchem \u4e0b\u8f7dUni-Mol # \u5b89\u88c5Uni-Core wget https://github.com/dptech-corp/Uni-Core/archive/refs/heads/main.zip unzip main.zip cd Uni-Core-main python setup.py install # \u5b89\u88c5Uni-Mol wget https://github.com/dptech-corp/Uni-Mol/archive/refs/heads/main.zip unzip main.zip cd Uni-Mol-main/unimol python setup.py install #\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd wget https://github.com/dptech-corp/Uni-Mol/releases/download/v0.1/mol_pre_no_h_220816.pt \u4e0b\u8f7dBBBP.csv\u6587\u4ef6 #\u4eceMoleculeNet\u4e0b\u8f7dBBBP.csv mkdir raw_data cd raw_data wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv \u6784\u5efa\u6570\u636e\u96c6 import deepchem as dc import pandas as pd import numpy from rdkit import Chem df = pd.read_csv(\"./raw_data/BBBP.csv\") # \u53bb\u9664\u4e0d\u80fd\u88abrdkit\u8bfb\u53d6\u7684\u5206\u5b50 ys = [] smiles = [] for label, smi in zip(df['p_np'].to_list(),df['smiles'].to_list()): mol = Chem.MolFromSmiles(smi) if mol is not None: ys.append(label) smiles.append(smi) # \u6784\u5efa\u65b0\u7684\u6570\u636e\u96c6 Xs = numpy.ones_like(ys) dataset = dc.data.NumpyDataset(X=Xs,y=ys,ids=smiles) # \u5212\u5206\u6570\u636e\u96c6 scaffoldsplitter = dc.splits.ScaffoldSplitter() train, valid, test = scaffoldsplitter.train_valid_test_split(dataset, 0.8, 0.1, 0.1) def dataset2df(dataset): data = {\"smiles\": list(dataset.ids), \"p_np\":list(dataset.y), } df = pd.DataFrame(data) return df # \u4fee\u6539\u4e86write_lmdb\u51fd\u6570\uff0c\u5c06\u4f20\u5165\u53d8\u91cf\u7531CSV\u6587\u4ef6\u53d8\u4e3a\u5df2\u7ecf\u7531deepChem\u5212\u5206\u597d\u7684\u6570\u636e\u96c6 def write_lmdb(train, valid, test, outpath='./', nthreads=16): train, valid, test = [dataset2df(dataset) for dataset in [train, valid, test]] for name, content_list in [('train.lmdb', zip(*[train[c].values.tolist() for c in train])), ('valid.lmdb', zip(*[valid[c].values.tolist() for c in valid])), ('test.lmdb', zip(*[test[c].values.tolist() for c in test]))]: os.makedirs(outpath, exist_ok=True) output_name = os.path.join(outpath, name) try: os.remove(output_name) except: pass env_new = lmdb.open( output_name, subdir=False, readonly=False, lock=False, readahead=False, meminit=False, max_readers=1, map_size=int(100e9), ) txn_write = env_new.begin(write=True) with Pool(nthreads) as pool: i = 0 for inner_output in tqdm(pool.imap(smi2coords, content_list)): if inner_output is not None: txn_write.put(f'{i}'.encode(\"ascii\"), inner_output) i += 1 print('{} process {} lines'.format(name, i)) txn_write.commit() env_new.close() write_lmdb(train, valid, test, outpath='./BBBP', nthreads=8) \u7ed3\u5408\u8d85\u53c2\u6570\u641c\u7d22\u7684finetune \u4e3a\u4e86\u4f7f\u5f97\u6a21\u578b\u83b7\u5f97\u6700\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5728BBBP\u8bad\u7ec3\u96c6\u4e0a\u8fdb\u884c\u53c2\u6570\u4f18\u5316\u7684\u540c\u65f6\uff0c\u9488\u5bf9\u5b66\u4e60\u7387lr\u3001\u6279\u6837\u672c\u6570\u3001\u8bad\u7ec3\u8f6e\u6570\u3001dropout\u3001wramup\u8fd95\u4e2a\u8d85\u53c2\u6570\u8fdb\u884c\u7f51\u683c\u641c\u7d22\u3002 data_path='./' # replace to your data path save_dir='./save_demo' # replace to your save path MASTER_PORT=10086 n_gpu=1 dict_name='dict.txt' weight_path='./weights/mol_pre_no_h_220816.pt' # replace to your ckpt path task_name='BBBP' # data folder name task_num=2 loss_func='finetune_cross_entropy' local_batch_size=32 only_polar=0 # -1 all h; 0 no h conf_size=11 seed=0 metric=\"valid_agg_auc\" cp example_data/molecule/$dict_name $data_path export NCCL_ASYNC_ERROR_HANDLING=1 export OMP_NUM_THREADS=1 for lr in 1e-4 4e-4 5e-4; do for batch_size in 64 128 256; do for epoch in 40 60; do for dropout in 0.0 0.1; do for warmup in 0.0 0.06 0.1; do update_freq=`expr $batch_size / $local_batch_size` save_dir='./save_demo/'$lr'_'$batch_size'_'$epoch'_'$dropout'_'$warmup python -m torch.distributed.launch --nproc_per_node=$n_gpu --master_port=$MASTER_PORT --use_env \\ $(which unicore-train) $data_path --task-name $task_name \\ --user-dir ../unimol --train-subset train --valid-subset valid \\ --conf-size $conf_size \\ --num-workers 8 --ddp-backend=c10d \\ --dict-name $dict_name \\ --task mol_finetune --loss $loss_func --arch unimol_base \\ --classification-head-name $task_name --num-classes $task_num \\ --optimizer adam --adam-betas '(0.9, 0.99)' --adam-eps 1e-6 --clip-norm 1.0 \\ --lr-scheduler polynomial_decay --lr $lr --warmup-ratio $warmup --max-epoch $epoch --batch-size $local_batch_size --pooler-dropout $dropout\\ --update-freq $update_freq --seed $seed \\ --fp16 --fp16-init-scale 4 --fp16-scale-window 256 \\ --log-interval 100 --log-format simple \\ --validate-interval 1 --keep-last-epochs 10 \\ --finetune-from-model $weight_path \\ --best-checkpoint-metric $metric --patience 20 \\ --save-dir $save_dir --only-polar $only_polar \\ --maximize-best-checkpoint-metric done done done done done \u57fa\u4e8eAUC\u7684\u6a21\u578b\u8bc4\u4f30 \u8d85\u53c2\u6570\u7f51\u683c\u641c\u7d22\u7684\u6574\u4f53\u8ba1\u7b97\u91cf\u6bd4\u8f83\u5927\uff0c\u6682\u65f6\u6ca1\u62ff\u5230\u5b8c\u6574\u7ed3\u679c\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u7528Uni-Mol\u7ed9\u51fa\u5173\u4e8eBBBP\u6a21\u578b\u7684\u8d85\u53c2\u6570\u6700\u4f18\u7ec4\u5408\uff1alr=4e-4\uff0cbatch_size=128\uff0cepoch=40\uff0cdropout=0\uff0cwarmup=0.06\uff0c\u8fdb\u884c\u4e86\u6a21\u578b\u5fae\u8c03\uff0c\u5f97\u5230\u4e86 \u9884\u6d4b\u7ed3\u679c(csv\u683c\u5f0f) \uff0cAUC=0.691\u3002","title":"\u9879\u76ee\u6d41\u7a0b\u4e0e\u7ed3\u679c"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/#_1","text":"","title":"\u9879\u76ee\u6d41\u7a0b\u4e0e\u7ed3\u679c"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/#_2","text":"\u65b0\u5efaAnaconda\u865a\u62df\u73af\u5883 conda create -n unimol python=3.8 conda activate unimol pip3 install torch torchvision torchaudio pip3 install rdkit pip3 install deepchem \u4e0b\u8f7dUni-Mol # \u5b89\u88c5Uni-Core wget https://github.com/dptech-corp/Uni-Core/archive/refs/heads/main.zip unzip main.zip cd Uni-Core-main python setup.py install # \u5b89\u88c5Uni-Mol wget https://github.com/dptech-corp/Uni-Mol/archive/refs/heads/main.zip unzip main.zip cd Uni-Mol-main/unimol python setup.py install #\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd wget https://github.com/dptech-corp/Uni-Mol/releases/download/v0.1/mol_pre_no_h_220816.pt \u4e0b\u8f7dBBBP.csv\u6587\u4ef6 #\u4eceMoleculeNet\u4e0b\u8f7dBBBP.csv mkdir raw_data cd raw_data wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv \u6784\u5efa\u6570\u636e\u96c6 import deepchem as dc import pandas as pd import numpy from rdkit import Chem df = pd.read_csv(\"./raw_data/BBBP.csv\") # \u53bb\u9664\u4e0d\u80fd\u88abrdkit\u8bfb\u53d6\u7684\u5206\u5b50 ys = [] smiles = [] for label, smi in zip(df['p_np'].to_list(),df['smiles'].to_list()): mol = Chem.MolFromSmiles(smi) if mol is not None: ys.append(label) smiles.append(smi) # \u6784\u5efa\u65b0\u7684\u6570\u636e\u96c6 Xs = numpy.ones_like(ys) dataset = dc.data.NumpyDataset(X=Xs,y=ys,ids=smiles) # \u5212\u5206\u6570\u636e\u96c6 scaffoldsplitter = dc.splits.ScaffoldSplitter() train, valid, test = scaffoldsplitter.train_valid_test_split(dataset, 0.8, 0.1, 0.1) def dataset2df(dataset): data = {\"smiles\": list(dataset.ids), \"p_np\":list(dataset.y), } df = pd.DataFrame(data) return df # \u4fee\u6539\u4e86write_lmdb\u51fd\u6570\uff0c\u5c06\u4f20\u5165\u53d8\u91cf\u7531CSV\u6587\u4ef6\u53d8\u4e3a\u5df2\u7ecf\u7531deepChem\u5212\u5206\u597d\u7684\u6570\u636e\u96c6 def write_lmdb(train, valid, test, outpath='./', nthreads=16): train, valid, test = [dataset2df(dataset) for dataset in [train, valid, test]] for name, content_list in [('train.lmdb', zip(*[train[c].values.tolist() for c in train])), ('valid.lmdb', zip(*[valid[c].values.tolist() for c in valid])), ('test.lmdb', zip(*[test[c].values.tolist() for c in test]))]: os.makedirs(outpath, exist_ok=True) output_name = os.path.join(outpath, name) try: os.remove(output_name) except: pass env_new = lmdb.open( output_name, subdir=False, readonly=False, lock=False, readahead=False, meminit=False, max_readers=1, map_size=int(100e9), ) txn_write = env_new.begin(write=True) with Pool(nthreads) as pool: i = 0 for inner_output in tqdm(pool.imap(smi2coords, content_list)): if inner_output is not None: txn_write.put(f'{i}'.encode(\"ascii\"), inner_output) i += 1 print('{} process {} lines'.format(name, i)) txn_write.commit() env_new.close() write_lmdb(train, valid, test, outpath='./BBBP', nthreads=8)","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/#finetune","text":"\u4e3a\u4e86\u4f7f\u5f97\u6a21\u578b\u83b7\u5f97\u6700\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5728BBBP\u8bad\u7ec3\u96c6\u4e0a\u8fdb\u884c\u53c2\u6570\u4f18\u5316\u7684\u540c\u65f6\uff0c\u9488\u5bf9\u5b66\u4e60\u7387lr\u3001\u6279\u6837\u672c\u6570\u3001\u8bad\u7ec3\u8f6e\u6570\u3001dropout\u3001wramup\u8fd95\u4e2a\u8d85\u53c2\u6570\u8fdb\u884c\u7f51\u683c\u641c\u7d22\u3002 data_path='./' # replace to your data path save_dir='./save_demo' # replace to your save path MASTER_PORT=10086 n_gpu=1 dict_name='dict.txt' weight_path='./weights/mol_pre_no_h_220816.pt' # replace to your ckpt path task_name='BBBP' # data folder name task_num=2 loss_func='finetune_cross_entropy' local_batch_size=32 only_polar=0 # -1 all h; 0 no h conf_size=11 seed=0 metric=\"valid_agg_auc\" cp example_data/molecule/$dict_name $data_path export NCCL_ASYNC_ERROR_HANDLING=1 export OMP_NUM_THREADS=1 for lr in 1e-4 4e-4 5e-4; do for batch_size in 64 128 256; do for epoch in 40 60; do for dropout in 0.0 0.1; do for warmup in 0.0 0.06 0.1; do update_freq=`expr $batch_size / $local_batch_size` save_dir='./save_demo/'$lr'_'$batch_size'_'$epoch'_'$dropout'_'$warmup python -m torch.distributed.launch --nproc_per_node=$n_gpu --master_port=$MASTER_PORT --use_env \\ $(which unicore-train) $data_path --task-name $task_name \\ --user-dir ../unimol --train-subset train --valid-subset valid \\ --conf-size $conf_size \\ --num-workers 8 --ddp-backend=c10d \\ --dict-name $dict_name \\ --task mol_finetune --loss $loss_func --arch unimol_base \\ --classification-head-name $task_name --num-classes $task_num \\ --optimizer adam --adam-betas '(0.9, 0.99)' --adam-eps 1e-6 --clip-norm 1.0 \\ --lr-scheduler polynomial_decay --lr $lr --warmup-ratio $warmup --max-epoch $epoch --batch-size $local_batch_size --pooler-dropout $dropout\\ --update-freq $update_freq --seed $seed \\ --fp16 --fp16-init-scale 4 --fp16-scale-window 256 \\ --log-interval 100 --log-format simple \\ --validate-interval 1 --keep-last-epochs 10 \\ --finetune-from-model $weight_path \\ --best-checkpoint-metric $metric --patience 20 \\ --save-dir $save_dir --only-polar $only_polar \\ --maximize-best-checkpoint-metric done done done done done","title":"\u7ed3\u5408\u8d85\u53c2\u6570\u641c\u7d22\u7684finetune"},{"location":"%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C/#auc","text":"\u8d85\u53c2\u6570\u7f51\u683c\u641c\u7d22\u7684\u6574\u4f53\u8ba1\u7b97\u91cf\u6bd4\u8f83\u5927\uff0c\u6682\u65f6\u6ca1\u62ff\u5230\u5b8c\u6574\u7ed3\u679c\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u7528Uni-Mol\u7ed9\u51fa\u5173\u4e8eBBBP\u6a21\u578b\u7684\u8d85\u53c2\u6570\u6700\u4f18\u7ec4\u5408\uff1alr=4e-4\uff0cbatch_size=128\uff0cepoch=40\uff0cdropout=0\uff0cwarmup=0.06\uff0c\u8fdb\u884c\u4e86\u6a21\u578b\u5fae\u8c03\uff0c\u5f97\u5230\u4e86 \u9884\u6d4b\u7ed3\u679c(csv\u683c\u5f0f) \uff0cAUC=0.691\u3002","title":"\u57fa\u4e8eAUC\u7684\u6a21\u578b\u8bc4\u4f30"}]}